{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8d14558-a868-43d5-a503-af47d768f7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /hongyi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /hongyi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /hongyi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /hongyi/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'stopwords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maveraged_perceptron_tagger\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# 获取中文停用词列表\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m stopwords_cn_list \u001b[38;5;241m=\u001b[39m \u001b[43mstopwords\u001b[49m\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchinese\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stopwords' is not defined"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2ad7f018-fe4f-4c27-84fe-b74ceb01219e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding dimension: 768\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('/hongyi/stream/sentence-transformers/all-mpnet-base-v2/')\n",
    "embedding_dim = model.get_sentence_embedding_dimension()\n",
    "print(f\"embedding dimension: {embedding_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c13123ef-567c-462a-8487-5164c6b86b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text labels\n",
      "0     黄蜂vs湖人首发：科比带伤战保罗 加索尔救赎之战 新浪体育讯北京时间4月27日，NBA季后赛...     体育\n",
      "1     1.7秒神之一击救马刺王朝于危难 这个新秀有点牛！新浪体育讯在刚刚结束的比赛中，回到主场的马...     体育\n",
      "2     1人灭掘金！神般杜兰特！ 他想要分的时候没人能挡新浪体育讯在NBA的世界里，真的猛男，敢于直...     体育\n",
      "3     韩国国奥20人名单：朴周永领衔 两世界杯国脚入选新浪体育讯据韩联社首尔9月17日电 韩国国奥...     体育\n",
      "4     天才中锋崇拜王治郅 周琦：球员最终是靠实力说话2月14日从土耳其男篮邀请赛回到北京之后，周琦...     体育\n",
      "...                                                 ...    ...\n",
      "4995  五基金同发分红公告早报讯 (记者孟梅 实习生夏冬梅) 在近期包括本报在内的媒体对部分基金不愿...     财经\n",
      "4996  今年基金高管变动增多2009年，A股市场一路反弹至2500点上方。虽然股指反弹幅度已经超过3...     财经\n",
      "4997  偏股基金单季加仓9个百分点整体股票仓位回升至熊市初期水平，“领先”沪指近一千点⊙本报记者 弘...     财经\n",
      "4998  创投基金被盗第一案原告胜诉获赔4月20日，黑龙江辰能哈工大高科技风险投资有限公司发起设立了黑...     财经\n",
      "4999  华夏大盘逆势减仓近5%大多数基金一季度加仓□晨报记者 李锐在一片加仓声中，一季度减仓的基金备...     财经\n",
      "\n",
      "[5000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 文件路径\n",
    "file_path = \"/hongyi/stream/dataset/cnews.val.txt\"\n",
    "\n",
    "# 创建一个空列表来存储处理后的数据\n",
    "data = []\n",
    "\n",
    "# 打开文件并读取每一行\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        # 确保每一行至少有4个字符\n",
    "        if len(line) > 3:\n",
    "            # 提取前两个字符作为第一个元素\n",
    "            first_two_chars = line[:2].strip()\n",
    "            # 提取从第四个字开始的剩余部分作为第二个元素\n",
    "            remaining_chars = line[3:].strip()\n",
    "            # 将两个元素添加到列表中\n",
    "            data.append([remaining_chars, first_two_chars])\n",
    "\n",
    "# 创建 DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"text\", \"labels\"])\n",
    "\n",
    "# 显示 DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f654747-b333-446c-9e33-d198ac687033",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hongyi/anaconda3/envs/mystream/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "Preprocessing documents:   0%|                                                                | 0/1000 [00:00<?, ?it/s]Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.699 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Preprocessing documents: 100%|████████████████████████████████████████████████████| 1000/1000 [00:08<00:00, 124.08it/s]\n",
      "\u001b[32m2024-11-11 15:09:24.955\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstream_topic.utils.dataset\u001b[0m:\u001b[36mcreate_load_save_dataset\u001b[0m:\u001b[36m239\u001b[0m - \u001b[1mDataset saved to /hongyi/stream/dataset/my_dataset.parquet\u001b[0m\n",
      "\u001b[32m2024-11-11 15:09:24.968\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstream_topic.utils.dataset\u001b[0m:\u001b[36mcreate_load_save_dataset\u001b[0m:\u001b[36m254\u001b[0m - \u001b[1mDataset info saved to /hongyi/stream/dataset/my_dataset_info.pkl\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  language                              stopwords_path  \\\n",
      "0    zh-cn  /hongyi/stream/stopwords/scu_stopwords.txt   \n",
      "1    zh-cn  /hongyi/stream/stopwords/scu_stopwords.txt   \n",
      "2    zh-cn  /hongyi/stream/stopwords/scu_stopwords.txt   \n",
      "3    zh-cn  /hongyi/stream/stopwords/scu_stopwords.txt   \n",
      "4    zh-cn  /hongyi/stream/stopwords/scu_stopwords.txt   \n",
      "\n",
      "                                                text labels  \n",
      "0                                            加索尔 加索尔     体育  \n",
      "1      体育讯 波波维奇 季后赛 名不见经传 土耳其 名不见经传 土耳其 西班牙 西班牙 波波维奇     体育  \n",
      "2  杜兰特 体育讯 季后赛 杜兰特 所有人 杜兰特 三分球 杜兰特 拉塞尔 斯布鲁克 杜兰特 詹...     体育  \n",
      "3  世界杯 体育讯 国奥队 亚运会 足球赛 国奥队 世界杯 金正友 亚运会 足球赛 金正友 亚运...     体育  \n",
      "4  王治郅 土耳其 邀请赛 土耳其 周琦说 邀请赛 西班牙 周琦说 土耳其 土耳其 第二个 第二...     体育  \n"
     ]
    }
   ],
   "source": [
    "#本段落用时9min\n",
    "from stream_topic.utils.dataset import TMDataset\n",
    "\n",
    "# 创建 TMDataset 实例\n",
    "dataset = TMDataset(language=\"zh-cn\", stopwords_path = '/hongyi/stream/stopwords/scu_stopwords.txt')\n",
    "\n",
    "# 假设你有一个名为 df 的 DataFrame，包含你的数据\n",
    "# df = pd.DataFrame(...)\n",
    "\n",
    "# 数据集名称\n",
    "dataset_name = \"my_dataset\"\n",
    "\n",
    "# 指定保存数据集的目录\n",
    "save_dir = \"/hongyi/stream/dataset\"\n",
    "df = df.iloc[0:1000]\n",
    "# 调用 create_load_save_dataset 方法来存储数据集\n",
    "dataset.create_load_save_dataset(\n",
    "    data=df,\n",
    "    dataset_name=dataset_name,\n",
    "    save_dir=save_dir,\n",
    "    doc_column=\"text\",  # 假设 DataFrame 中包含文本的列名为 \"text_column\"\n",
    "    label_column=\"labels\",  # 假设 DataFrame 中包含标签的列名为 \"label_column\"\n",
    "    language = \"zh-cn\",\n",
    "    stopwords_path = '/hongyi/stream/stopwords/scu_stopwords.txt'\n",
    ")\n",
    "\n",
    "# 打印保存的数据集信息\n",
    "print(dataset.dataframe.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63f379d2-ab0a-487b-adbb-4db045454a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zh-cn\n",
      "zh-cn\n"
     ]
    }
   ],
   "source": [
    "from langdetect import detect\n",
    "print(detect(df.text[1]))\n",
    "print(detect(df.text[609]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b3d8c0b-12d7-4179-956d-9ec693e1e3a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-12 15:41:00.536\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstream_topic.utils.dataset\u001b[0m:\u001b[36mfetch_dataset\u001b[0m:\u001b[36m120\u001b[0m - \u001b[1mFetching dataset: my_dataset\u001b[0m\n",
      "\u001b[32m2024-11-12 15:41:00.539\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstream_topic.utils.dataset\u001b[0m:\u001b[36mfetch_dataset\u001b[0m:\u001b[36m130\u001b[0m - \u001b[1mFetching dataset from local path\u001b[0m\n",
      "Preprocessing documents: 100%|███████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1562.34it/s]\n",
      "model_head.pkl not found in /hongyi/stream/sentence-transformers/paraphrase-multilingual-mpnet-base-v2, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "\u001b[32m2024-11-12 15:41:03.140\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstream_topic.models.DCTE\u001b[0m:\u001b[36mfit\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1m--- Preparing paraphrase-MiniLM-L3-v2 Dataset ---\u001b[0m\n",
      "\u001b[32m2024-11-12 15:41:03.161\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstream_topic.models.DCTE\u001b[0m:\u001b[36mfit\u001b[0m:\u001b[36m212\u001b[0m - \u001b[1m--- Training DCTE topic model ---\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- a train-validation split of 0.8 to 0.2 is performed --- \n",
      "---change 'val_split' if needed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44241e6894f14769b4a6ad757a5a5f09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num unique pairs = 16000\n",
      "  Batch size = 6\n",
      "  Num epochs = 10\n",
      "  Total optimization steps = 26670\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='517' max='26670' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  517/26670 00:44 < 37:23, 11.66 it/s, Epoch 0.19/0]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-12 15:41:49.222\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mstream_topic.models.DCTE\u001b[0m:\u001b[36mfit\u001b[0m:\u001b[36m230\u001b[0m - \u001b[31m\u001b[1mTraining interrupted.\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#本段落用时4h\u001b[39;00m\n\u001b[1;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m DCTE(local_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/hongyi/stream/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/\u001b[39m\u001b[38;5;124m\"\u001b[39m,stopwords_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/hongyi/stream/stopwords/scu_stopwords.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;66;03m# \u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#, n_topics=10\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# model = NMFTM(stopwords_path = '/hongyi/stream/stopwords/scu_stopwords.txt')# \u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# model.fit(dataset)#, n_topics=10\u001b[39;00m\n\u001b[1;32m     14\u001b[0m topics \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_topics()\n",
      "File \u001b[0;32m~/STREAM/stream_topic/models/DCTE.py:221\u001b[0m, in \u001b[0;36mDCTE.fit\u001b[0;34m(self, dataset, val_split, **training_args)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer \u001b[38;5;241m=\u001b[39m SetfitTrainer(\n\u001b[1;32m    214\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m    215\u001b[0m     args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m    216\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_ds,\n\u001b[1;32m    217\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_ds,\n\u001b[1;32m    218\u001b[0m )\n\u001b[1;32m    220\u001b[0m \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[0;32m--> 221\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;66;03m# evaluate accuracy\u001b[39;00m\n\u001b[1;32m    223\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "File \u001b[0;32m~/anaconda3/envs/mystream/lib/python3.10/site-packages/setfit/trainer.py:410\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, args, trial, **kwargs)\u001b[0m\n\u001b[1;32m    405\u001b[0m train_parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_to_parameters(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataset)\n\u001b[1;32m    406\u001b[0m full_parameters \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    407\u001b[0m     train_parameters \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_to_parameters(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_dataset) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_dataset \u001b[38;5;28;01melse\u001b[39;00m train_parameters\n\u001b[1;32m    408\u001b[0m )\n\u001b[0;32m--> 410\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfull_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_classifier(\u001b[38;5;241m*\u001b[39mtrain_parameters, args\u001b[38;5;241m=\u001b[39margs)\n",
      "File \u001b[0;32m~/anaconda3/envs/mystream/lib/python3.10/site-packages/setfit/trainer.py:462\u001b[0m, in \u001b[0;36mTrainer.train_embeddings\u001b[0;34m(self, x_train, y_train, x_eval, y_eval, args)\u001b[0m\n\u001b[1;32m    459\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Total optimization steps = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_train_steps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    461\u001b[0m warmup_steps \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mceil(total_train_steps \u001b[38;5;241m*\u001b[39m args\u001b[38;5;241m.\u001b[39mwarmup_proportion)\n\u001b[0;32m--> 462\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_sentence_transformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mystream/lib/python3.10/site-packages/setfit/trainer.py:625\u001b[0m, in \u001b[0;36mTrainer._train_sentence_transformer\u001b[0;34m(self, model_body, train_dataloader, eval_dataloader, args, loss_func, warmup_steps)\u001b[0m\n\u001b[1;32m    622\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(data_iterator)\n\u001b[1;32m    624\u001b[0m features, labels \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m--> 625\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    626\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m batch: batch_to_device(batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice), features))\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39muse_amp:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from stream_topic.models import KmeansTM,BERTopicTM,CBC,DCTE,NMFTM,SOMTM\n",
    "from stream_topic.utils import TMDataset\n",
    "\n",
    "#本段落用时9min\n",
    "dataset = TMDataset(language=\"zh-cn\",stopwords_path = '/hongyi/stream/stopwords/scu_stopwords.txt')# \n",
    "dataset.fetch_dataset(name = \"my_dataset\", dataset_path = \"/hongyi/stream/dataset\", source = 'local')\n",
    "dataset.preprocess(model_type=\"KmeansTM\", min_word_length = 1)\n",
    "#本段落用时4h\n",
    "model = DCTE(local_model_path = \"/hongyi/stream/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/\",stopwords_path = '/hongyi/stream/stopwords/scu_stopwords.txt')# \n",
    "model.fit(dataset)#, n_topics=10\n",
    "# model = NMFTM(stopwords_path = '/hongyi/stream/stopwords/scu_stopwords.txt')# \n",
    "# model.fit(dataset)#, n_topics=10\n",
    "\n",
    "topics = model.get_topics()\n",
    "print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38a637ef-84dc-4cd4-b7dc-2675a1db0407",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stream_topic.metrics import ISIM, INT, ISH,Expressivity, NPMI, Embedding_Coherence, Embedding_Topic_Diversity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from stream_topic.metrics.metrics_config import MetricsConfig\n",
    "MetricsConfig.set_PARAPHRASE_embedder(\"/hongyi/stream/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/\")#paraphrase-multilingual-mpnet-base-v2\n",
    "MetricsConfig.set_SENTENCE_embedder(\"/hongyi/stream/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/\")#all-mpnet-base-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0390c890-960c-495a-8d8f-d4cba43aebbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from local path: /hongyi/stream/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/\n",
      "Loading model from local path: /hongyi/stream/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/\n",
      "ISIM scores: 0.5655106644415194\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "score_list=[]\n",
    "metric = ISIM()\n",
    "for i in range(100):    \n",
    "    scores = metric.score(topics) #值越小越好\n",
    "    score_list.append(scores)\n",
    "print(\"ISIM scores:\", np.mean(score_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81a30645-3f8e-4cd4-8c11-ec4f73a91cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from local path: /hongyi/stream/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/\n",
      "Loading model from local path: /hongyi/stream/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/\n",
      "INT scores: 0.19\n"
     ]
    }
   ],
   "source": [
    "score_list=[]\n",
    "metric = INT()\n",
    "for i in range(100):    \n",
    "    scores = metric.score(topics) #值越小越好\n",
    "    score_list.append(scores)\n",
    "print(\"INT scores:\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31263cff-4e54-4bce-8c8f-20acb3695460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from local path: /hongyi/stream/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/\n",
      "Loading model from local path: /hongyi/stream/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/\n",
      "ISH scores: 0.7455579936504364\n"
     ]
    }
   ],
   "source": [
    "score_list=[]\n",
    "metric = ISH()\n",
    "for i in range(100):    \n",
    "    scores = metric.score(topics) #值越小越好\n",
    "    score_list.append(scores)\n",
    "print(\"ISH scores:\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b63a5ddd-24dc-4cc7-90dd-1c564c98df28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def load_stopwords(stopwords_path):\n",
    "        # load Chinese stopwords list\n",
    "        return pd.read_csv(stopwords_path, names=['w'], sep='\\t', encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91d509de-0100-4c23-84c2-c8c8497d6ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from local path: /hongyi/stream/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/\n",
      "Loading model from local path: /hongyi/stream/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/\n",
      "Loading model from local path: /hongyi/stream/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/\n",
      "Metric Info: {'metric_name': 'Expressivity', 'n_words': 5, 'embedding_model_name': SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: XLMRobertaModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "), 'metric_range': '0 to 1, smaller is better', 'description': 'The expressivity metric measures the distance between the mean vector of the top words in a topic and the mean vector of the embeddings of the stop words.'}\n",
      "Expressivity scores: 0.5395905000000001\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# topics = [[\"apple\", \"banana\", \"cherry\", \"date\", \"fig\",\"apple\", \"banana\", \"cherry\", \"date\", \"fig\",\"apple\", \"banana\", \"cherry\", \"date\", \"fig\"],\n",
    "#           [\"dog\", \"cat\", \"rabbit\", \"hamster\", \"gerbil\",\"dog\", \"cat\", \"rabbit\", \"hamster\", \"gerbil\",\"dog\", \"cat\", \"rabbit\", \"hamster\", \"gerbil\"]]\n",
    "stopword = load_stopwords('/hongyi/stream/stopwords/scu_stopwords.txt')\n",
    "expressivity_metric = Expressivity(\n",
    "n_words=5,\n",
    "custom_stopwords=list(stopword)\n",
    ")\n",
    "beta = np.random.rand(20, 384)\n",
    "info = expressivity_metric.get_info()\n",
    "print(\"Metric Info:\", info)\n",
    "scores = expressivity_metric.score(topics, beta)  #值越小越好\n",
    "print(\"Expressivity scores:\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "558722b9-fb9a-4380-8275-cba087333bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from local path: /hongyi/stream/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/\n",
      "Loading model from local path: /hongyi/stream/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/\n",
      "Metric Info: {'metric_name': 'Embedding Topic Diversity', 'n_words': 10, 'embedding_model_name': SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: XLMRobertaModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "), 'metric_range': '0 to 1, smaller is better', 'description': 'The diversity metric measures the mean cosine similarity of the mean vectors of the top words of all topics.'}\n",
      "Diversity score: 0.8043285156357554\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "beta = np.random.rand(10, 384)\n",
    "diversity_metric = Embedding_Topic_Diversity()\n",
    "info = diversity_metric.get_info()\n",
    "print(\"Metric Info:\", info)\n",
    "scores = diversity_metric.score(topics, beta)  #值越小越好\n",
    "print(\"Diversity score:\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9dfa2fd9-1955-4b78-b50d-bb500b954457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.99063"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = NPMI(dataset,language = \"chinese\") #值越大越好\n",
    "metric.score(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bb9174ad-74ca-4b26-a411-bad9599fa3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from local path: /hongyi/stream/sentence-transformers/paraphrase-MiniLM-L3-v2/\n",
      "1\n",
      "Loading model from local path: /hongyi/stream/sentence-transformers/paraphrase-MiniLM-L3-v2/\n",
      "Coherence scores per topic: {'我要评论, 据了解, 同时, 因此, 此外': 0.71686, 'eos, 参考价格, 编辑点评, 编辑观点, 联系电话': 0.62031, '吴哥, 组图, 报道, 我要评论, 此外': 0.83146, '主持人, 微博, 新浪娱乐, 记者, 所以': 0.81639, '微博, com, 封神ol, 失落时空ii, 封神online': 0.43578, '组图, 时尚点评, 新浪体育, 因此, 当然': 0.87777, '微博, 记者, 平方米, 此外, 他说': 0.69242, '新浪家居, 28美元, cric联席董事长, cric的成功上市, 易居中国与新浪合资公司中国房产信息集团': 0.66137, '其中, 平方米, 我要评论, 不过, 旗下楼盘': 0.77129, 'pipgame, 武林ol, 明珠三国, 幻想i时代, wl': 0.37291}\n",
      "Overall coherence score: 0.679656\n"
     ]
    }
   ],
   "source": [
    "metric = Embedding_Coherence()\n",
    "topic_scores = metric.score_per_topic(topics)\n",
    "print(\"Coherence scores per topic:\", topic_scores)\n",
    "overall_score = metric.score(topics)\n",
    "print(\"Overall coherence score:\", overall_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "91122cef-0a6f-4af1-9c78-3799d689bb9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'我要评论, 据了解, 同时, 因此, 此外': 0.71686,\n",
       " 'eos, 参考价格, 编辑点评, 编辑观点, 联系电话': 0.62031,\n",
       " '吴哥, 组图, 报道, 我要评论, 此外': 0.83146,\n",
       " '主持人, 微博, 新浪娱乐, 记者, 所以': 0.81639,\n",
       " '微博, com, 封神ol, 失落时空ii, 封神online': 0.43578,\n",
       " '组图, 时尚点评, 新浪体育, 因此, 当然': 0.87777,\n",
       " '微博, 记者, 平方米, 此外, 他说': 0.69242,\n",
       " '新浪家居, 28美元, cric联席董事长, cric的成功上市, 易居中国与新浪合资公司中国房产信息集团': 0.66137,\n",
       " '其中, 平方米, 我要评论, 不过, 旗下楼盘': 0.77129,\n",
       " 'pipgame, 武林ol, 明珠三国, 幻想i时代, wl': 0.37291}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.score_per_topic(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a726ec12-651d-465d-b41a-6b6092eb3b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.74\n"
     ]
    }
   ],
   "source": [
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity\n",
    "\n",
    "model_output = {\"topics\": model.get_topics(), \"topic-word-matrix\": model.get_beta(), \"topic-document-matrix\": model.get_theta()}\n",
    "\n",
    "metric = TopicDiversity(topk=10) # Initialize metric\n",
    "topic_diversity_score = metric.score(model_output)\n",
    "print(topic_diversity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3357b04d-d22c-4f05-93ac-de875cf63886",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [11:08<00:00, 14.96it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "label_mapping = {\n",
    "    '体育': 7,\n",
    "    '娱乐': 9,\n",
    "    '家居': 5,\n",
    "    '房产': 4,\n",
    "    '教育': 0,\n",
    "    '时尚': 6,\n",
    "    '时政': 2,\n",
    "    '游戏': 3,\n",
    "    '科技': 8,\n",
    "    '财经': 1\n",
    "}\n",
    "\n",
    "predictions = model.predict(dataset.texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "935b9c5d-dd94-4223-9d09-2049784b5ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设 dataset.labels 包含所有样本的真实标签\n",
    "true_labels = dataset.labels\n",
    "\n",
    "# 将汉字类别转换为数值类别\n",
    "true_labels_numeric = [label_mapping[label] for label in true_labels]\n",
    "\n",
    "# 将预测结果和真实标签转换为 DataFrame\n",
    "df_predictions = pd.DataFrame(predictions, columns=['predicted_labels'])\n",
    "df_true_labels = pd.DataFrame(true_labels_numeric, columns=['true_labels'])\n",
    "\n",
    "# 计算混淆矩阵\n",
    "conf_matrix = confusion_matrix(df_true_labels, df_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "51253e45-dfe3-461c-b6c3-adc9a23c8cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 accuracy: 0.10\n",
      "Topic 1 accuracy: 0.00\n",
      "Topic 2 accuracy: 0.11\n",
      "Topic 3 accuracy: 0.11\n",
      "Topic 4 accuracy: 0.00\n",
      "Topic 5 accuracy: 0.69\n",
      "Topic 6 accuracy: 0.31\n",
      "Topic 7 accuracy: 0.00\n",
      "Topic 8 accuracy: 0.02\n",
      "Topic 9 accuracy: 0.00\n"
     ]
    }
   ],
   "source": [
    "# 计算每个类别的准确率\n",
    "accuracies = {}\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    accuracy = conf_matrix[i, i] / conf_matrix.sum(axis=1)[i]\n",
    "    accuracies[f'Topic {i}'] = accuracy\n",
    "\n",
    "# 打印每个类别的准确率\n",
    "for topic, accuracy in accuracies.items():\n",
    "    print(f'{topic} accuracy: {accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe2dd060-6979-46b1-9664-32622e99f6b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[ 12  17   4   1   0 875  42  11  25  13]\n",
      " [  0   5   0   0 981   1   1   0   2  10]\n",
      " [  2 873   1   0   5  41   7   0  24  47]\n",
      " [973   0   0   1   1   2  12   4   7   0]\n",
      " [  1  44   0   0   5   2   9   1  97 841]\n",
      " [  3  16   2   1   4   2  22  10 906  34]\n",
      " [  4   0   0   0   0   0  15 962  19   0]\n",
      " [  1   4 985   0   0   0   8   0   2   0]\n",
      " [  1   3   0 980   0   0   4   0  11   1]\n",
      " [  1   6   1   1   0   2 982   6   0   1]]\n"
     ]
    }
   ],
   "source": [
    "# 如果你想要更详细的信息，可以打印整个混淆矩阵\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5a9f9fff-3ce0-4954-912c-eccf9e754f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[102   2 371 131   1 357   5   0  31   0]\n",
      " [ 53   1  62 322 436 104  15   0   7   0]\n",
      " [403   0 113 192   2 177  92   8  13   0]\n",
      " [312   1 107 108   8  47 204   0 213   0]\n",
      " [121  45  27 119   3  46 607   0  32   0]\n",
      " [ 44  16 154  75  16 685   5   0   5   0]\n",
      " [404   0 202  25   5  12 313   0  39   0]\n",
      " [ 54  12  32 145 618  51  11   0  11  66]\n",
      " [ 14 863  76   4   3  20   4   0  16   0]\n",
      " [ 64   0 210  26   1  68  32   0 599   0]]\n"
     ]
    }
   ],
   "source": [
    "# 如果你想要更详细的信息，可以打印整个混淆矩阵\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mystream",
   "language": "python",
   "name": "mystream"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
