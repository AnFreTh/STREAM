{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8d14558-a868-43d5-a503-af47d768f7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /hongyi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /hongyi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /hongyi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /hongyi/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'stopwords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maveraged_perceptron_tagger\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# 获取中文停用词列表\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m stopwords_cn_list \u001b[38;5;241m=\u001b[39m \u001b[43mstopwords\u001b[49m\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchinese\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stopwords' is not defined"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2ad7f018-fe4f-4c27-84fe-b74ceb01219e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding dimension: 768\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('/hongyi/stream/sentence-transformers/all-mpnet-base-v2/')\n",
    "embedding_dim = model.get_sentence_embedding_dimension()\n",
    "print(f\"embedding dimension: {embedding_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c13123ef-567c-462a-8487-5164c6b86b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text labels\n",
      "0     鲍勃库西奖归谁属？ NCAA最强控卫是坎巴还是弗神新浪体育讯如今，本赛季的NCAA进入到了末...     体育\n",
      "1     麦基砍28+18+5却充满寂寞 纪录之夜他的痛阿联最懂新浪体育讯上天对每个人都是公平的，贾维...     体育\n",
      "2     黄蜂vs湖人首发：科比冲击七连胜 火箭两旧将登场新浪体育讯北京时间3月28日，NBA常规赛洛...     体育\n",
      "3     双面谢亚龙作秀终成做作 谁来为低劣行政能力埋单是谁任命了谢亚龙？谁放纵了谢亚龙？谁又该为谢亚...     体育\n",
      "4     兔年首战山西换帅后有虎胆 张学文用乔丹名言励志今晚客场挑战浙江稠州银行队，是山西汾酒男篮的兔...     体育\n",
      "...                                                 ...    ...\n",
      "9995  近期18只偏股基金成立 将为股市新增300亿资金兴业有机增长混合基金23日发布公告称，基金募...     财经\n",
      "9996  银华基金杨靖聊政策性主题投资机会实录新浪财经讯 银华和谐主题基金拟任基金经理助理杨靖于3月2...     财经\n",
      "9997  首只基金投资信心指数问世本报讯 (记者吴敏)昨日，嘉实基金宣布推出“嘉实中国基金投资者信心指...     财经\n",
      "9998  17只阳光私募3月份火速成立证券时报记者 方 丽本报讯 阳光私募产品迎来了发行高潮。WIND...     财经\n",
      "9999  25日股票基金全线受挫 九成半基金跌逾1%全景网3月26日讯 周三开放式基金净值普降，股票型...     财经\n",
      "\n",
      "[10000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 文件路径\n",
    "file_path = \"/hongyi/stream/dataset/cnews.test.txt\"\n",
    "\n",
    "# 创建一个空列表来存储处理后的数据\n",
    "data = []\n",
    "\n",
    "# 打开文件并读取每一行\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        # 确保每一行至少有4个字符\n",
    "        if len(line) > 3:\n",
    "            # 提取前两个字符作为第一个元素\n",
    "            first_two_chars = line[:2].strip()\n",
    "            # 提取从第四个字开始的剩余部分作为第二个元素\n",
    "            remaining_chars = line[3:].strip()\n",
    "            # 将两个元素添加到列表中\n",
    "            data.append([remaining_chars, first_two_chars])\n",
    "\n",
    "# 创建 DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"text\", \"labels\"])\n",
    "\n",
    "# 显示 DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f654747-b333-446c-9e33-d198ac687033",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hongyi/anaconda3/envs/mystream/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "Preprocessing documents:   0%|                                                                     | 0/10000 [00:00<?, ?it/s]Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.522 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Preprocessing documents: 100%|████████████████████████████████████████████████████████| 10000/10000 [01:37<00:00, 102.40it/s]\n",
      "\u001b[32m2024-11-21 14:52:06.547\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstream_topic.utils.dataset\u001b[0m:\u001b[36mcreate_load_save_dataset\u001b[0m:\u001b[36m239\u001b[0m - \u001b[1mDataset saved to /hongyi/stream/dataset/my_dataset.parquet\u001b[0m\n",
      "\u001b[32m2024-11-21 14:52:06.578\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstream_topic.utils.dataset\u001b[0m:\u001b[36mcreate_load_save_dataset\u001b[0m:\u001b[36m254\u001b[0m - \u001b[1mDataset info saved to /hongyi/stream/dataset/my_dataset_info.pkl\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  language                                stopwords_path  min_word_length  \\\n",
      "0    zh-cn  /hongyi/stream/stopwords/baidu_stopwords.txt                1   \n",
      "1    zh-cn  /hongyi/stream/stopwords/baidu_stopwords.txt                1   \n",
      "2    zh-cn  /hongyi/stream/stopwords/baidu_stopwords.txt                1   \n",
      "3    zh-cn  /hongyi/stream/stopwords/baidu_stopwords.txt                1   \n",
      "4    zh-cn  /hongyi/stream/stopwords/baidu_stopwords.txt                1   \n",
      "\n",
      "                                                text labels  \n",
      "0  鲍勃 库西 控卫 坎巴 弗神 本赛季 评选 即将 评选 最佳 控卫 鲍勃 库西 最终 鲍勃 ...     体育  \n",
      "1  麦基 砍 却 阿联 最 新浪 体育讯 人 都 麦基 不 奇才 客场 勇士 麦基 上半场 表现...     体育  \n",
      "2  黄蜂 湖人 首发 科比 新浪 体育讯 月 日 湖人 主场 黄蜂 赛前 公布 首发 阵容 点击...     体育  \n",
      "3  谢亚龙 作秀 做作 低劣 行政 能力 埋单 任命 谢亚龙 放纵 谢亚龙 谢亚龙 南勇们 倒掉...     体育  \n",
      "4  兔年 首战 换帅 后 张学文 乔丹 名言 今晚 客场 队 山西汾酒 兔年 上 两队 山西队 ...     体育  \n"
     ]
    }
   ],
   "source": [
    "#本段落用时9min\n",
    "from stream_topic.utils.dataset import TMDataset\n",
    "\n",
    "# 创建 TMDataset 实例\n",
    "dataset = TMDataset()#language=\"zh-cn\", stopwords_path = '/hongyi/stream/stopwords/baidu_stopwords.txt'\n",
    "\n",
    "# 假设你有一个名为 df 的 DataFrame，包含你的数据\n",
    "# df = pd.DataFrame(...)\n",
    "\n",
    "# 数据集名称\n",
    "dataset_name = \"my_dataset\"\n",
    "\n",
    "# 指定保存数据集的目录\n",
    "save_dir = \"/hongyi/stream/dataset\"\n",
    "# df = df.iloc[0:100]\n",
    "# 调用 create_load_save_dataset 方法来存储数据集\n",
    "dataset.create_load_save_dataset(\n",
    "    data=df,\n",
    "    dataset_name=dataset_name,\n",
    "    save_dir=save_dir,\n",
    "    doc_column=\"text\",  # 假设 DataFrame 中包含文本的列名为 \"text_column\"\n",
    "    label_column=\"labels\",  # 假设 DataFrame 中包含标签的列名为 \"label_column\"\n",
    "    language = \"zh-cn\",\n",
    "    stopwords_path = '/hongyi/stream/stopwords/baidu_stopwords.txt',\n",
    "    min_word_length = 1\n",
    ")\n",
    "\n",
    "# 打印保存的数据集信息\n",
    "print(dataset.dataframe.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b08909c-71d4-4f49-9cb9-c979a83a86cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series中不存在英文字符\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "# 创建一个示例的Series\n",
    "data = dataset.dataframe.text\n",
    "# 定义一个函数来检查字符串中是否包含英文字符\n",
    "def contains_english(text):\n",
    "    return bool(re.search('[a-zA-Z]', text))\n",
    "# 在Series中应用函数，检查每个元素是否包含英文字符\n",
    "contains_english_series = data.apply(contains_english)\n",
    "# 检查是否有英文字符存在\n",
    "if contains_english_series.any():\n",
    "    print(\"Series中存在英文字符\")\n",
    "else:\n",
    "    print(\"Series中不存在英文字符\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b3d8c0b-12d7-4179-956d-9ec693e1e3a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-21 14:52:17.328\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstream_topic.utils.dataset\u001b[0m:\u001b[36mfetch_dataset\u001b[0m:\u001b[36m120\u001b[0m - \u001b[1mFetching dataset: my_dataset\u001b[0m\n",
      "\u001b[32m2024-11-21 14:52:17.329\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstream_topic.utils.dataset\u001b[0m:\u001b[36mfetch_dataset\u001b[0m:\u001b[36m130\u001b[0m - \u001b[1mFetching dataset from local path\u001b[0m\n",
      "Preprocessing documents: 100%|████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 1110250.41it/s]\n",
      "\u001b[32m2024-11-21 14:52:18.011\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstream_topic.models.KmeansTM\u001b[0m:\u001b[36mfit\u001b[0m:\u001b[36m212\u001b[0m - \u001b[1m--- Training KmeansTM topic model ---\u001b[0m\n",
      "\u001b[32m2024-11-21 14:58:34.196\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstream_topic.models.abstract_helper_models.base\u001b[0m:\u001b[36mprepare_embeddings\u001b[0m:\u001b[36m225\u001b[0m - \u001b[1m--- Creating /hongyi/stream/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/ document embeddings ---\u001b[0m\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10000/10000 [10:08<00:00, 16.43it/s]\n",
      "\u001b[32m2024-11-21 15:08:45.018\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstream_topic.models.abstract_helper_models.base\u001b[0m:\u001b[36mdim_reduction\u001b[0m:\u001b[36m196\u001b[0m - \u001b[1m--- Reducing dimensions ---\u001b[0m\n",
      "\u001b[32m2024-11-21 15:09:15.905\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstream_topic.models.KmeansTM\u001b[0m:\u001b[36m_clustering\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1m--- Creating document cluster ---\u001b[0m\n",
      "\u001b[32m2024-11-21 15:09:34.736\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstream_topic.models.KmeansTM\u001b[0m:\u001b[36mfit\u001b[0m:\u001b[36m281\u001b[0m - \u001b[1m--- Training completed successfully. ---\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['家具', '地板', '家居', '消费者', '品牌', '装修', '家装', '橱柜', '卖场', '产品'], ['比赛', '球队', '篮板', '球员', '季后赛', '赛季', '热火', '火箭', '进攻', '防守'], ['像素', '机身', '英寸', '采用', '光学', '功能', '相机', '佳能', '索尼', '镜头'], ['搭配', '性感', '导语', '时尚', '黑色', '外套', '组图', '白色', '款式', '单品'], ['股票', '型基金', '债券', '指数', '收益', '分红', '投资者', '净值', '经理', '仓位'], ['留学', '移民', '大学', '学生', '学校', '申请', '签证', '考试', '留学生', '教育'], ['导演', '电影', '影片', '观众', '票房', '演员', '微博', '娱乐', '上映', '角色'], ['玩家', '游戏', '手机', '网游', '奖励', '装备', '客服', '活动', '封神', '手机游戏'], ['房价', '房地产', '土地', '楼市', '开发商', '住房', '平方米', '楼盘', '成交', '地产'], ['主席', '合作', '马英九', '台湾', '陈水扁', '胡锦涛', '中方', '两岸', '日电', '流感']]\n"
     ]
    }
   ],
   "source": [
    "from stream_topic.models import KmeansTM,BERTopicTM,CBC,DCTE,NMFTM,SOMTM\n",
    "from stream_topic.utils import TMDataset\n",
    "\n",
    "#本段落用时9min\n",
    "dataset = TMDataset()# \n",
    "dataset.fetch_dataset(name = \"my_dataset\", dataset_path = \"/hongyi/stream/dataset\", source = 'local')\n",
    "dataset.preprocess(model_type=\"KmeansTM\", language=\"zh-cn\", stopwords_path = '/hongyi/stream/stopwords/baidu_stopwords.txt', min_word_length = 1)\n",
    "#本段落用时4h\n",
    "model = KmeansTM(\"/hongyi/stream/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/\",stopwords_path = '/hongyi/stream/stopwords/baidu_stopwords.txt')# \n",
    "model.fit(dataset, n_topics=10)#\n",
    "# model = NMFTM(stopwords_path = '/hongyi/stream/stopwords/scu_stopwords.txt')# \n",
    "# model.fit(dataset)#, n_topics=10\n",
    "\n",
    "topics = model.get_topics()\n",
    "print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38a637ef-84dc-4cd4-b7dc-2675a1db0407",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stream_topic.metrics import ISIM, INT, ISH,Expressivity, NPMI, Embedding_Coherence, Embedding_Topic_Diversity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from stream_topic.metrics.metrics_config import MetricsConfig\n",
    "MetricsConfig.set_PARAPHRASE_embedder(\"/hongyi/stream/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/\")#paraphrase-multilingual-mpnet-base-v2\n",
    "MetricsConfig.set_SENTENCE_embedder(\"/hongyi/stream/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/\")#all-mpnet-base-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0390c890-960c-495a-8d8f-d4cba43aebbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from local path: /hongyi/stream/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/\n",
      "Loading model from local path: /hongyi/stream/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/\n",
      "ISIM scores: 0.5434089997410775\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "score_list=[]\n",
    "metric = ISIM()\n",
    "for i in range(100):    \n",
    "    scores = metric.score(topics) #值越小越好\n",
    "    score_list.append(scores)\n",
    "print(\"ISIM scores:\", np.mean(score_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81a30645-3f8e-4cd4-8c11-ec4f73a91cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from local path: /hongyi/stream/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/\n",
      "Loading model from local path: /hongyi/stream/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/\n",
      "INT scores: 0.4800000000000001\n"
     ]
    }
   ],
   "source": [
    "score_list=[]\n",
    "metric = INT()\n",
    "for i in range(100):    \n",
    "    scores = metric.score(topics) #值越小越好\n",
    "    score_list.append(scores)\n",
    "print(\"INT scores:\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31263cff-4e54-4bce-8c8f-20acb3695460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from local path: /hongyi/stream/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/\n",
      "Loading model from local path: /hongyi/stream/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/\n",
      "ISH scores: 0.6459499955177307\n"
     ]
    }
   ],
   "source": [
    "score_list=[]\n",
    "metric = ISH()\n",
    "for i in range(100):    \n",
    "    scores = metric.score(topics) #值越小越好\n",
    "    score_list.append(scores)\n",
    "print(\"ISH scores:\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b63a5ddd-24dc-4cc7-90dd-1c564c98df28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def load_stopwords(stopwords_path):\n",
    "        # load Chinese stopwords list\n",
    "        return pd.read_csv(stopwords_path, names=['w'], sep='\\t', encoding='UTF-8')\n",
    "stopword = load_stopwords('/hongyi/stream/stopwords/scu_stopwords.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91d509de-0100-4c23-84c2-c8c8497d6ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from local path: /hongyi/stream/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/\n",
      "Loading model from local path: /hongyi/stream/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/\n",
      "Loading model from local path: /hongyi/stream/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/\n",
      "Metric Info: {'metric_name': 'Expressivity', 'n_words': 5, 'embedding_model_name': SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: XLMRobertaModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "), 'metric_range': '0 to 1, smaller is better', 'description': 'The expressivity metric measures the distance between the mean vector of the top words in a topic and the mean vector of the embeddings of the stop words.'}\n",
      "Expressivity scores: 0.511857\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# topics = [[\"apple\", \"banana\", \"cherry\", \"date\", \"fig\",\"apple\", \"banana\", \"cherry\", \"date\", \"fig\",\"apple\", \"banana\", \"cherry\", \"date\", \"fig\"],\n",
    "#           [\"dog\", \"cat\", \"rabbit\", \"hamster\", \"gerbil\",\"dog\", \"cat\", \"rabbit\", \"hamster\", \"gerbil\",\"dog\", \"cat\", \"rabbit\", \"hamster\", \"gerbil\"]]\n",
    "expressivity_metric = Expressivity(\n",
    "n_words=5,\n",
    "custom_stopwords=list(stopword)\n",
    ")\n",
    "beta = np.random.rand(10, 384)\n",
    "info = expressivity_metric.get_info()\n",
    "print(\"Metric Info:\", info)\n",
    "scores = expressivity_metric.score(topics, beta)  #值越小越好\n",
    "print(\"Expressivity scores:\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dfa2fd9-1955-4b78-b50d-bb500b954457",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'tokens'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/mystream/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'tokens'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m metric \u001b[38;5;241m=\u001b[39m \u001b[43mNPMI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzh-cn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_stopwords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstopword\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#值越大越好\u001b[39;00m\n\u001b[1;32m      2\u001b[0m metric\u001b[38;5;241m.\u001b[39mscore(topics)\n",
      "File \u001b[0;32m~/STREAM/stream_topic/metrics/coherence_metrics.py:82\u001b[0m, in \u001b[0;36mNPMI.__init__\u001b[0;34m(self, dataset, language, custom_stopwords)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlanguage \u001b[38;5;241m=\u001b[39m language\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset \u001b[38;5;241m=\u001b[39m dataset\n\u001b[0;32m---> 82\u001b[0m files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_corpus\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfiles \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(words) \u001b[38;5;28;01mfor\u001b[39;00m words \u001b[38;5;129;01min\u001b[39;00m files]\n",
      "File \u001b[0;32m~/STREAM/stream_topic/utils/dataset.py:450\u001b[0m, in \u001b[0;36mTMDataset.get_corpus\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_corpus\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    442\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;124;03m    Get the corpus (tokens) from the dataframe.\u001b[39;00m\n\u001b[1;32m    444\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;124;03m        Corpus tokens.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataframe\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m~/anaconda3/envs/mystream/lib/python3.10/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/anaconda3/envs/mystream/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'tokens'"
     ]
    }
   ],
   "source": [
    "metric = NPMI(dataset,language = \"zh-cn\", custom_stopwords=list(stopword)) #值越大越好\n",
    "metric.score(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07930a90-d514-486d-8b58-cb7af5cdfccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"我\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "558722b9-fb9a-4380-8275-cba087333bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from local path: /hongyi/stream/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/\n",
      "Loading model from local path: /hongyi/stream/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/\n",
      "Metric Info: {'metric_name': 'Embedding Topic Diversity', 'n_words': 10, 'embedding_model_name': SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: XLMRobertaModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "), 'metric_range': '0 to 1, smaller is better', 'description': 'The diversity metric measures the mean cosine similarity of the mean vectors of the top words of all topics.'}\n",
      "Diversity score: 0.7283671072973759\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "beta = np.random.rand(10, 384)\n",
    "diversity_metric = Embedding_Topic_Diversity()\n",
    "info = diversity_metric.get_info()\n",
    "print(\"Metric Info:\", info)\n",
    "scores = diversity_metric.score(topics, beta)  #值越小越好\n",
    "print(\"Diversity score:\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bb9174ad-74ca-4b26-a411-bad9599fa3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from local path: /hongyi/stream/sentence-transformers/paraphrase-MiniLM-L3-v2/\n",
      "1\n",
      "Loading model from local path: /hongyi/stream/sentence-transformers/paraphrase-MiniLM-L3-v2/\n",
      "Coherence scores per topic: {'我要评论, 据了解, 同时, 因此, 此外': 0.71686, 'eos, 参考价格, 编辑点评, 编辑观点, 联系电话': 0.62031, '吴哥, 组图, 报道, 我要评论, 此外': 0.83146, '主持人, 微博, 新浪娱乐, 记者, 所以': 0.81639, '微博, com, 封神ol, 失落时空ii, 封神online': 0.43578, '组图, 时尚点评, 新浪体育, 因此, 当然': 0.87777, '微博, 记者, 平方米, 此外, 他说': 0.69242, '新浪家居, 28美元, cric联席董事长, cric的成功上市, 易居中国与新浪合资公司中国房产信息集团': 0.66137, '其中, 平方米, 我要评论, 不过, 旗下楼盘': 0.77129, 'pipgame, 武林ol, 明珠三国, 幻想i时代, wl': 0.37291}\n",
      "Overall coherence score: 0.679656\n"
     ]
    }
   ],
   "source": [
    "metric = Embedding_Coherence()\n",
    "topic_scores = metric.score_per_topic(topics)\n",
    "print(\"Coherence scores per topic:\", topic_scores)\n",
    "overall_score = metric.score(topics)\n",
    "print(\"Overall coherence score:\", overall_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "91122cef-0a6f-4af1-9c78-3799d689bb9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'我要评论, 据了解, 同时, 因此, 此外': 0.71686,\n",
       " 'eos, 参考价格, 编辑点评, 编辑观点, 联系电话': 0.62031,\n",
       " '吴哥, 组图, 报道, 我要评论, 此外': 0.83146,\n",
       " '主持人, 微博, 新浪娱乐, 记者, 所以': 0.81639,\n",
       " '微博, com, 封神ol, 失落时空ii, 封神online': 0.43578,\n",
       " '组图, 时尚点评, 新浪体育, 因此, 当然': 0.87777,\n",
       " '微博, 记者, 平方米, 此外, 他说': 0.69242,\n",
       " '新浪家居, 28美元, cric联席董事长, cric的成功上市, 易居中国与新浪合资公司中国房产信息集团': 0.66137,\n",
       " '其中, 平方米, 我要评论, 不过, 旗下楼盘': 0.77129,\n",
       " 'pipgame, 武林ol, 明珠三国, 幻想i时代, wl': 0.37291}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.score_per_topic(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a726ec12-651d-465d-b41a-6b6092eb3b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.74\n"
     ]
    }
   ],
   "source": [
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity\n",
    "\n",
    "model_output = {\"topics\": model.get_topics(), \"topic-word-matrix\": model.get_beta(), \"topic-document-matrix\": model.get_theta()}\n",
    "\n",
    "metric = TopicDiversity(topk=10) # Initialize metric\n",
    "topic_diversity_score = metric.score(model_output)\n",
    "print(topic_diversity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3357b04d-d22c-4f05-93ac-de875cf63886",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|██████████████████████████████████████████████████████████████████████████████████▍| 9938/10000 [09:51<00:04, 15.00it/s]"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "label_mapping = {\n",
    "    '体育': 7,\n",
    "    '娱乐': 9,\n",
    "    '家居': 5,\n",
    "    '房产': 4,\n",
    "    '教育': 0,\n",
    "    '时尚': 6,\n",
    "    '时政': 2,\n",
    "    '游戏': 3,\n",
    "    '科技': 8,\n",
    "    '财经': 1\n",
    "}\n",
    "\n",
    "predictions = model.predict(dataset.texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "935b9c5d-dd94-4223-9d09-2049784b5ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设 dataset.labels 包含所有样本的真实标签\n",
    "true_labels = dataset.labels\n",
    "\n",
    "# 将汉字类别转换为数值类别\n",
    "true_labels_numeric = [label_mapping[label] for label in true_labels]\n",
    "\n",
    "# 将预测结果和真实标签转换为 DataFrame\n",
    "df_predictions = pd.DataFrame(predictions, columns=['predicted_labels'])\n",
    "df_true_labels = pd.DataFrame(true_labels_numeric, columns=['true_labels'])\n",
    "\n",
    "# 计算混淆矩阵\n",
    "conf_matrix = confusion_matrix(df_true_labels, df_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "51253e45-dfe3-461c-b6c3-adc9a23c8cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 accuracy: 0.10\n",
      "Topic 1 accuracy: 0.00\n",
      "Topic 2 accuracy: 0.11\n",
      "Topic 3 accuracy: 0.11\n",
      "Topic 4 accuracy: 0.00\n",
      "Topic 5 accuracy: 0.69\n",
      "Topic 6 accuracy: 0.31\n",
      "Topic 7 accuracy: 0.00\n",
      "Topic 8 accuracy: 0.02\n",
      "Topic 9 accuracy: 0.00\n"
     ]
    }
   ],
   "source": [
    "# 计算每个类别的准确率\n",
    "accuracies = {}\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    accuracy = conf_matrix[i, i] / conf_matrix.sum(axis=1)[i]\n",
    "    accuracies[f'Topic {i}'] = accuracy\n",
    "\n",
    "# 打印每个类别的准确率\n",
    "for topic, accuracy in accuracies.items():\n",
    "    print(f'{topic} accuracy: {accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe2dd060-6979-46b1-9664-32622e99f6b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[  2   1   0   0 496   0   0   0   0   1]\n",
      " [  6   0   0   0   0   0   0  25 469   0]\n",
      " [ 36   3   0   1  12   0   1 435   5   7]\n",
      " [  0 487   0   3   2   1   2   5   0   0]\n",
      " [199   2   0   0   1   1   3  21  10 263]\n",
      " [435   4   0   1   0  17   4  20   0  19]\n",
      " [  3   3   1   1   0 472  17   2   0   1]\n",
      " [  1   4   0 489   2   0   0   4   0   0]\n",
      " [  2   8 487   0   0   0   1   0   0   2]\n",
      " [  0   4   2   1   0   5 480   8   0   0]]\n"
     ]
    }
   ],
   "source": [
    "# 如果你想要更详细的信息，可以打印整个混淆矩阵\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5a9f9fff-3ce0-4954-912c-eccf9e754f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[102   2 371 131   1 357   5   0  31   0]\n",
      " [ 53   1  62 322 436 104  15   0   7   0]\n",
      " [403   0 113 192   2 177  92   8  13   0]\n",
      " [312   1 107 108   8  47 204   0 213   0]\n",
      " [121  45  27 119   3  46 607   0  32   0]\n",
      " [ 44  16 154  75  16 685   5   0   5   0]\n",
      " [404   0 202  25   5  12 313   0  39   0]\n",
      " [ 54  12  32 145 618  51  11   0  11  66]\n",
      " [ 14 863  76   4   3  20   4   0  16   0]\n",
      " [ 64   0 210  26   1  68  32   0 599   0]]\n"
     ]
    }
   ],
   "source": [
    "# 如果你想要更详细的信息，可以打印整个混淆矩阵\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mystream",
   "language": "python",
   "name": "mystream"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
