{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104e9527-cc1b-40b2-8330-a17ac372f97f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a11de3-2339-4125-a7d1-23572bd6baf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbff20b9-1dfe-4eff-a4ee-48d6461cf3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text labels\n",
      "0     鲍勃库西奖归谁属？ NCAA最强控卫是坎巴还是弗神新浪体育讯如今，本赛季的NCAA进入到了末...     体育\n",
      "1     麦基砍28+18+5却充满寂寞 纪录之夜他的痛阿联最懂新浪体育讯上天对每个人都是公平的，贾维...     体育\n",
      "2     黄蜂vs湖人首发：科比冲击七连胜 火箭两旧将登场新浪体育讯北京时间3月28日，NBA常规赛洛...     体育\n",
      "3     双面谢亚龙作秀终成做作 谁来为低劣行政能力埋单是谁任命了谢亚龙？谁放纵了谢亚龙？谁又该为谢亚...     体育\n",
      "4     兔年首战山西换帅后有虎胆 张学文用乔丹名言励志今晚客场挑战浙江稠州银行队，是山西汾酒男篮的兔...     体育\n",
      "...                                                 ...    ...\n",
      "9995  近期18只偏股基金成立 将为股市新增300亿资金兴业有机增长混合基金23日发布公告称，基金募...     财经\n",
      "9996  银华基金杨靖聊政策性主题投资机会实录新浪财经讯 银华和谐主题基金拟任基金经理助理杨靖于3月2...     财经\n",
      "9997  首只基金投资信心指数问世本报讯 (记者吴敏)昨日，嘉实基金宣布推出“嘉实中国基金投资者信心指...     财经\n",
      "9998  17只阳光私募3月份火速成立证券时报记者 方 丽本报讯 阳光私募产品迎来了发行高潮。WIND...     财经\n",
      "9999  25日股票基金全线受挫 九成半基金跌逾1%全景网3月26日讯 周三开放式基金净值普降，股票型...     财经\n",
      "\n",
      "[10000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# 文件路径\n",
    "file_path = \"/hongyi/stream/dataset/cnews.test.txt\"\n",
    "# 创建一个空列表来存储处理后的数据\n",
    "data = []\n",
    "# 打开文件并读取每一行\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        # 确保每一行至少有4个字符\n",
    "        if len(line) > 3:\n",
    "            # 提取前两个字符作为第一个元素\n",
    "            first_two_chars = line[:2].strip()\n",
    "            # 提取从第四个字开始的剩余部分作为第二个元素\n",
    "            remaining_chars = line[3:].strip()\n",
    "            # 将两个元素添加到列表中\n",
    "            data.append([remaining_chars, first_two_chars])\n",
    "\n",
    "# 创建 DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"text\", \"labels\"])\n",
    "# 显示 DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a66cd46c-2cac-4111-a895-c7915a517634",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hongyi/anaconda3/envs/mystream/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "Preprocessing documents:   0%|                                                               | 0/10000 [00:00<?, ?it/s]Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.567 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Preprocessing documents: 100%|██████████████████████████████████████████████████| 10000/10000 [01:27<00:00, 113.75it/s]\n",
      "\u001b[32m2024-11-11 14:39:58.331\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstream_topic.utils.dataset\u001b[0m:\u001b[36mcreate_load_save_dataset\u001b[0m:\u001b[36m239\u001b[0m - \u001b[1mDataset saved to /hongyi/stream/dataset/my_dataset.parquet\u001b[0m\n",
      "\u001b[32m2024-11-11 14:39:58.372\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstream_topic.utils.dataset\u001b[0m:\u001b[36mcreate_load_save_dataset\u001b[0m:\u001b[36m254\u001b[0m - \u001b[1mDataset info saved to /hongyi/stream/dataset/my_dataset_info.pkl\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  language                              stopwords_path  \\\n",
      "0    zh-cn  /hongyi/stream/stopwords/scu_stopwords.txt   \n",
      "1    zh-cn  /hongyi/stream/stopwords/scu_stopwords.txt   \n",
      "2    zh-cn  /hongyi/stream/stopwords/scu_stopwords.txt   \n",
      "3    zh-cn  /hongyi/stream/stopwords/scu_stopwords.txt   \n",
      "4    zh-cn  /hongyi/stream/stopwords/scu_stopwords.txt   \n",
      "\n",
      "                                                text labels  \n",
      "0  本赛季 史密斯 吉梅尔 杨百翰 吉梅尔 领导者 杨百翰 主教练 主教练 史密斯 杜克大学 史...     体育  \n",
      "1            体育讯 上半场 上半场 上半场 惠特曼 惠特曼 本赛季 下半场 上半场 下半场     体育  \n",
      "2                                        体育讯 直播室 直播室     体育  \n",
      "3  谢亚龙 谢亚龙 谢亚龙 谢亚龙 南勇们 谢亚龙 掌门人 谢亚龙 立案侦查 蔚少辉 李冬生 谢...     体育  \n",
      "4  张学文 山西汾酒 山西队 浙江队 山西汾酒 山西队 浙江队 山西队 山西队 浙江队 山西队 ...     体育  \n"
     ]
    }
   ],
   "source": [
    "#本段落用时9min\n",
    "from stream_topic.utils.dataset import TMDataset\n",
    "\n",
    "# 创建 TMDataset 实例\n",
    "dataset = TMDataset(language=\"zh-cn\", stopwords_path = '/hongyi/stream/stopwords/scu_stopwords.txt')\n",
    "# 假设你有一个名为 df 的 DataFrame，包含你的数据\n",
    "# df = pd.DataFrame(...)\n",
    "\n",
    "# 数据集名称\n",
    "dataset_name = \"my_dataset\"\n",
    "# df = df.iloc[0:100]\n",
    "# 指定保存数据集的目录\n",
    "save_dir = \"/hongyi/stream/dataset\"\n",
    "# df = df.iloc[0:1000]\n",
    "# 调用 create_load_save_dataset 方法来存储数据集\n",
    "dataset.create_load_save_dataset(\n",
    "    data=df,\n",
    "    dataset_name=dataset_name,\n",
    "    save_dir=save_dir,\n",
    "    doc_column=\"text\",  # 假设 DataFrame 中包含文本的列名为 \"text_column\"\n",
    "    label_column=\"labels\",  # 假设 DataFrame 中包含标签的列名为 \"label_column\"\n",
    "    language = \"zh-cn\",\n",
    "    stopwords_path = '/hongyi/stream/stopwords/scu_stopwords.txt'\n",
    ")\n",
    "# 打印保存的数据集信息\n",
    "print(dataset.dataframe.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a7e6c5e-d10d-4bc0-8680-1ac82bd65042",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-11 14:46:33.810\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstream_topic.utils.dataset\u001b[0m:\u001b[36mfetch_dataset\u001b[0m:\u001b[36m120\u001b[0m - \u001b[1mFetching dataset: my_dataset\u001b[0m\n",
      "\u001b[32m2024-11-11 14:46:33.811\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstream_topic.utils.dataset\u001b[0m:\u001b[36mfetch_dataset\u001b[0m:\u001b[36m130\u001b[0m - \u001b[1mFetching dataset from local path\u001b[0m\n",
      "Preprocessing documents: 100%|█████████████████████████████████████████████████| 10000/10000 [00:08<00:00, 1138.56it/s]\n",
      "\u001b[32m2024-11-11 14:46:49.969\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstream_topic.models.KmeansTM\u001b[0m:\u001b[36mfit\u001b[0m:\u001b[36m212\u001b[0m - \u001b[1m--- Training KmeansTM topic model ---\u001b[0m\n",
      "\u001b[32m2024-11-11 14:46:50.995\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstream_topic.models.abstract_helper_models.base\u001b[0m:\u001b[36mprepare_embeddings\u001b[0m:\u001b[36m225\u001b[0m - \u001b[1m--- Creating /hongyi/stream/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/ document embeddings ---\u001b[0m\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 10000/10000 [02:52<00:00, 57.99it/s]\n",
      "\u001b[32m2024-11-11 14:49:45.362\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstream_topic.models.abstract_helper_models.base\u001b[0m:\u001b[36mdim_reduction\u001b[0m:\u001b[36m196\u001b[0m - \u001b[1m--- Reducing dimensions ---\u001b[0m\n",
      "\u001b[32m2024-11-11 14:50:22.629\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstream_topic.models.KmeansTM\u001b[0m:\u001b[36m_clustering\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1m--- Creating document cluster ---\u001b[0m\n",
      "\u001b[32m2024-11-11 14:50:27.331\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mstream_topic.preprocessor._tf_idf\u001b[0m:\u001b[36mc_tf_idf\u001b[0m:\u001b[36m39\u001b[0m - \u001b[33m\u001b[1mNaNs or inf in tf matrix\u001b[0m\n",
      "\u001b[32m2024-11-11 14:50:27.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstream_topic.models.KmeansTM\u001b[0m:\u001b[36mfit\u001b[0m:\u001b[36m281\u001b[0m - \u001b[1m--- Training completed successfully. ---\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['电影节', '好莱坞', '预告片', '范冰冰', '张柏芝', '章子怡', '李连杰', '女主角', '五月天', '发布会'], ['数码相机', '传感器', '分辨率', '感应器', '显示屏', '液晶屏', '参考价格', '感光度', '变焦镜头', '处理器'], ['型基金', '投资者', '收益率', '今年以来', '封闭式', '基金净值', '易方达', '增长率', '货币基金', '混合型'], ['季后赛', '体育讯', '詹姆斯', '本赛季', '湖人队', '凯尔特人', '常规赛', '尼克斯', '加索尔', '安东尼'], ['留学生', '加拿大', '四六级', '毕业生', '申请人', '澳大利亚', '奖学金', '研究生', '新西兰', '新加坡'], ['连衣裙', '高跟鞋', '牛仔裤', '设计师', '针织衫', '比基尼', '女人味', '蝴蝶结', '羽绒服', '太阳镜'], ['房地产', '消费者', '开发商', '平方米', '二手房', '成交量', '万平方米', '木地板', '商品房', '购房者'], ['龚松林', '大酒店', '大错特错', '大门口', '大闹天宫', '大队人马', '大陆行', '大院里', '大雁塔', '大雅之堂'], ['手机游戏', '网络游戏', '官方网站', '女儿国', '掌上明珠', '让玩家', '客户端', '服务器', '充值卡', '资料片'], ['马英九', '陈水扁', '胡锦涛', '领导人', '国务院', '进一步', '金融危机', '温家宝', '俄罗斯', '中新网']]\n"
     ]
    }
   ],
   "source": [
    "from stream_topic.models import KmeansTM\n",
    "from stream_topic.utils import TMDataset\n",
    "\n",
    "#本段落用时9min\n",
    "dataset = TMDataset(language=\"zh-cn\",stopwords_path = '/hongyi/stream/stopwords/scu_stopwords.txt')# \n",
    "dataset.fetch_dataset(name = \"my_dataset\", dataset_path = \"/hongyi/stream/dataset\", source = 'local')\n",
    "dataset.preprocess(model_type=\"KmeansTM\", min_word_length = 1)\n",
    "#本段落用时4h\n",
    "model = KmeansTM('/hongyi/stream/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/',stopwords_path = '/hongyi/stream/stopwords/scu_stopwords.txt')# \n",
    "model.fit(dataset, n_topics=10)\n",
    "\n",
    "topics = model.get_topics()\n",
    "print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e28f6e85-8a37-47a4-8f5b-0092e4f202e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stream_topic.metrics import ISIM, INT, ISH,Expressivity, NPMI, Embedding_Coherence, Embedding_Topic_Diversity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from stream_topic.metrics.metrics_config import MetricsConfig\n",
    "MetricsConfig.set_PARAPHRASE_embedder(\"/hongyi/stream/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/\")#paraphrase-multilingual-mpnet-base-v2\n",
    "MetricsConfig.set_SENTENCE_embedder(\"/hongyi/stream/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/\")#all-mpnet-base-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "069c604f-f0ba-433e-aa65-414d05a76dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from local path: /hongyi/stream/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/\n",
      "Loading model from local path: /hongyi/stream/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/\n",
      "ISIM scores: 0.4583348022460938\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "score_list=[]\n",
    "metric = ISIM()\n",
    "for i in range(500):    \n",
    "    scores = metric.score(topics) #值越小越好\n",
    "    score_list.append(scores)\n",
    "print(\"ISIM scores:\", np.mean(score_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f77e9ec-b57e-42c3-be49-67f05a4d04af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from local path: /hongyi/stream/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/\n",
      "Loading model from local path: /hongyi/stream/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/\n",
      "INT scores: 0.24\n"
     ]
    }
   ],
   "source": [
    "score_list=[]\n",
    "metric = INT()\n",
    "for i in range(500):    \n",
    "    scores = metric.score(topics) #值越小越好\n",
    "    score_list.append(scores)\n",
    "print(\"INT scores:\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0e159b6-c74d-4912-9afd-ab0d745d1a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from local path: /hongyi/stream/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/\n",
      "Loading model from local path: /hongyi/stream/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/\n",
      "ISH scores: 0.6054579943418503\n"
     ]
    }
   ],
   "source": [
    "score_list=[]\n",
    "metric = ISH()\n",
    "for i in range(500):    \n",
    "    scores = metric.score(topics) #值越小越好\n",
    "    score_list.append(scores)\n",
    "print(\"ISH scores:\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5396d4b-8dea-4810-bf4d-20bdb46c0de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def load_stopwords(stopwords_path):\n",
    "        # load Chinese stopwords list\n",
    "        return pd.read_csv(stopwords_path, names=['w'], sep='\\t', encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90e49884-874b-45cd-bbcd-75aff49e94ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from local path: /hongyi/stream/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/\n",
      "Loading model from local path: /hongyi/stream/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/\n",
      "Loading model from local path: /hongyi/stream/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/\n",
      "Metric Info: {'metric_name': 'Expressivity', 'n_words': 5, 'embedding_model_name': SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: XLMRobertaModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "), 'metric_range': '0 to 1, smaller is better', 'description': 'The expressivity metric measures the distance between the mean vector of the top words in a topic and the mean vector of the embeddings of the stop words.'}\n",
      "Expressivity scores: 0.511005\n"
     ]
    }
   ],
   "source": [
    "stopword = load_stopwords('/hongyi/stream/stopwords/scu_stopwords.txt')\n",
    "expressivity_metric = Expressivity(\n",
    "n_words=5,\n",
    "custom_stopwords=list(stopword)\n",
    ")\n",
    "beta = np.random.rand(10, 384) #主题数，嵌入维度\n",
    "info = expressivity_metric.get_info()\n",
    "print(\"Metric Info:\", info)\n",
    "scores = expressivity_metric.score(topics, beta)  #值越小越好\n",
    "print(\"Expressivity scores:\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5b60734-4d0e-42a2-889f-516af2d85c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from local path: /hongyi/stream/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/\n",
      "Loading model from local path: /hongyi/stream/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/\n",
      "Metric Info: {'metric_name': 'Embedding Topic Diversity', 'n_words': 10, 'embedding_model_name': SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: XLMRobertaModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "), 'metric_range': '0 to 1, smaller is better', 'description': 'The diversity metric measures the mean cosine similarity of the mean vectors of the top words of all topics.'}\n",
      "Diversity score: 0.7194263865279577\n"
     ]
    }
   ],
   "source": [
    "beta = np.random.rand(10, 384)\n",
    "diversity_metric = Embedding_Topic_Diversity()\n",
    "info = diversity_metric.get_info()\n",
    "print(\"Metric Info:\", info)\n",
    "scores = diversity_metric.score(topics, beta)  #值越小越好\n",
    "print(\"Diversity score:\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03a74db9-a7db-4b1e-9b2c-cdfff91f18b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.95939"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = NPMI(dataset,language = \"chinese\") #值越大越好\n",
    "metric.score(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd743bba-a2dd-4537-aae6-481e94d7013c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from local path: /hongyi/stream/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/\n",
      "1\n",
      "Loading model from local path: /hongyi/stream/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/\n",
      "Coherence scores per topic: {'电影节, 好莱坞, 预告片, 范冰冰, 张柏芝': 0.66974, '数码相机, 传感器, 分辨率, 感应器, 显示屏': 0.56482, '型基金, 投资者, 收益率, 今年以来, 封闭式': 0.45252, '季后赛, 体育讯, 詹姆斯, 本赛季, 湖人队': 0.66196, '留学生, 加拿大, 四六级, 毕业生, 申请人': 0.48822, '连衣裙, 高跟鞋, 牛仔裤, 设计师, 针织衫': 0.58639, '房地产, 消费者, 开发商, 平方米, 二手房': 0.59031, '龚松林, 大酒店, 大错特错, 大门口, 大闹天宫': 0.59984, '手机游戏, 网络游戏, 官方网站, 女儿国, 掌上明珠': 0.5203, '马英九, 陈水扁, 胡锦涛, 领导人, 国务院': 0.5527}\n",
      "Overall coherence score: 0.56868\n"
     ]
    }
   ],
   "source": [
    "metric = Embedding_Coherence()\n",
    "topic_scores = metric.score_per_topic(topics)\n",
    "print(\"Coherence scores per topic:\", topic_scores)\n",
    "overall_score = metric.score(topics)\n",
    "print(\"Overall coherence score:\", overall_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1809b627-6afa-4102-9d6d-dd0e772e5b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'电影节, 好莱坞, 预告片, 范冰冰, 张柏芝': 0.66974,\n",
       " '数码相机, 传感器, 分辨率, 感应器, 显示屏': 0.56482,\n",
       " '型基金, 投资者, 收益率, 今年以来, 封闭式': 0.45252,\n",
       " '季后赛, 体育讯, 詹姆斯, 本赛季, 湖人队': 0.66196,\n",
       " '留学生, 加拿大, 四六级, 毕业生, 申请人': 0.48822,\n",
       " '连衣裙, 高跟鞋, 牛仔裤, 设计师, 针织衫': 0.58639,\n",
       " '房地产, 消费者, 开发商, 平方米, 二手房': 0.59031,\n",
       " '龚松林, 大酒店, 大错特错, 大门口, 大闹天宫': 0.59984,\n",
       " '手机游戏, 网络游戏, 官方网站, 女儿国, 掌上明珠': 0.5203,\n",
       " '马英九, 陈水扁, 胡锦涛, 领导人, 国务院': 0.5527}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.score_per_topic(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6269e907-b9e3-4143-b2a9-e73652eb4e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity\n",
    "\n",
    "model_output = {\"topics\": model.get_topics(), \"topic-word-matrix\": model.get_beta(), \"topic-document-matrix\": model.get_theta()}\n",
    "\n",
    "metric = TopicDiversity(topk=10) # Initialize metric\n",
    "topic_diversity_score = metric.score(model_output)\n",
    "print(topic_diversity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af242374-ed0c-4641-a864-23e29be62368",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 10000/10000 [02:47<00:00, 59.76it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "label_mapping = {\n",
    "    '体育': 7,\n",
    "    '娱乐': 9,\n",
    "    '家居': 5,\n",
    "    '房产': 4,\n",
    "    '教育': 0,\n",
    "    '时尚': 6,\n",
    "    '时政': 2,\n",
    "    '游戏': 3,\n",
    "    '科技': 8,\n",
    "    '财经': 1\n",
    "} #随便设置\n",
    "\n",
    "predictions = model.predict(dataset.texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "042b16d0-338b-43f6-9300-4ad2ee878118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[ 35   2   7   7 803  26  51   0   6  63]\n",
      " [  5   0 794   0   1   3  25   0   3 169]\n",
      " [ 15   0  11   1  40   2  43   0   5 883]\n",
      " [ 67   2   1  16   7  86  22   1 779  19]\n",
      " [  6   0  26   1   6  16 810   2   1 132]\n",
      " [ 33   3   4  12  11  61 790  11   5  70]\n",
      " [101   1   0   8   7 815  24  17  15  12]\n",
      " [  7   0   0 970   2   1   1   0   1  18]\n",
      " [  9 914   0   2   1  12  34   3  10  15]\n",
      " [922   0   0   5   2  20   4   1  11  35]]\n"
     ]
    }
   ],
   "source": [
    "# 假设 dataset.labels 包含所有样本的真实标签\n",
    "true_labels = dataset.labels\n",
    "\n",
    "# 将汉字类别转换为数值类别\n",
    "true_labels_numeric = [label_mapping[label] for label in true_labels]\n",
    "\n",
    "# 将预测结果和真实标签转换为 DataFrame\n",
    "df_predictions = pd.DataFrame(predictions, columns=['predicted_labels'])\n",
    "df_true_labels = pd.DataFrame(true_labels_numeric, columns=['true_labels'])\n",
    "\n",
    "# 计算混淆矩阵\n",
    "conf_matrix = confusion_matrix(df_true_labels, df_predictions)\n",
    "# 如果你想要更详细的信息，可以打印整个混淆矩阵\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mystream",
   "language": "python",
   "name": "mystream"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
