{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7eb8f18-53df-4ae1-8f61-dc7ff8c87702",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.524 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import jieba\n",
    "\n",
    "# 加载并分词处理中文停用词\n",
    "def load_stopwords(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        stopwords = f.read().splitlines()\n",
    "    return [\" \".join(jieba.cut(word)) for word in stopwords]\n",
    "\n",
    "# 加载预处理后的停用词\n",
    "stopwords = load_stopwords('/hongyi/stream/stopwords/baidu_stopwords.txt')\n",
    "\n",
    "# 使用 CountVectorizer 加载处理后的中文停用词\n",
    "vectorizer = CountVectorizer(stop_words=stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8b97b64-59d2-4396-964f-b741aea909a9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['--',\n",
       " '?',\n",
       " '“',\n",
       " '”',\n",
       " '》',\n",
       " '－ －',\n",
       " 'able',\n",
       " 'about',\n",
       " 'above',\n",
       " 'according',\n",
       " 'accordingly',\n",
       " 'across',\n",
       " 'actually',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " \"ain ' t\",\n",
       " 'all',\n",
       " 'allow',\n",
       " 'allows',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'an',\n",
       " 'and',\n",
       " 'another',\n",
       " 'any',\n",
       " 'anybody',\n",
       " 'anyhow',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anyways',\n",
       " 'anywhere',\n",
       " 'apart',\n",
       " 'appear',\n",
       " 'appreciate',\n",
       " 'appropriate',\n",
       " 'are',\n",
       " \"aren ' t\",\n",
       " 'around',\n",
       " 'as',\n",
       " \"a ' s\",\n",
       " 'aside',\n",
       " 'ask',\n",
       " 'asking',\n",
       " 'associated',\n",
       " 'at',\n",
       " 'available',\n",
       " 'away',\n",
       " 'awfully',\n",
       " 'be',\n",
       " 'became',\n",
       " 'because',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'been',\n",
       " 'before',\n",
       " 'beforehand',\n",
       " 'behind',\n",
       " 'being',\n",
       " 'believe',\n",
       " 'below',\n",
       " 'beside',\n",
       " 'besides',\n",
       " 'best',\n",
       " 'better',\n",
       " 'between',\n",
       " 'beyond',\n",
       " 'both',\n",
       " 'brief',\n",
       " 'but',\n",
       " 'by',\n",
       " 'came',\n",
       " 'can',\n",
       " 'cannot',\n",
       " 'cant',\n",
       " \"can ' t\",\n",
       " 'cause',\n",
       " 'causes',\n",
       " 'certain',\n",
       " 'certainly',\n",
       " 'changes',\n",
       " 'clearly',\n",
       " \"c ' mon\",\n",
       " 'co',\n",
       " 'com',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'concerning',\n",
       " 'consequently',\n",
       " 'consider',\n",
       " 'considering',\n",
       " 'contain',\n",
       " 'containing',\n",
       " 'contains',\n",
       " 'corresponding',\n",
       " 'could',\n",
       " \"couldn ' t\",\n",
       " 'course',\n",
       " \"c ' s\",\n",
       " 'currently',\n",
       " 'definitely',\n",
       " 'described',\n",
       " 'despite',\n",
       " 'did',\n",
       " \"didn ' t\",\n",
       " 'different',\n",
       " 'do',\n",
       " 'does',\n",
       " \"doesn ' t\",\n",
       " 'doing',\n",
       " 'done',\n",
       " \"don ' t\",\n",
       " 'down',\n",
       " 'downwards',\n",
       " 'during',\n",
       " 'each',\n",
       " 'edu',\n",
       " 'eg',\n",
       " 'eight',\n",
       " 'either',\n",
       " 'else',\n",
       " 'elsewhere',\n",
       " 'enough',\n",
       " 'entirely',\n",
       " 'especially',\n",
       " 'et',\n",
       " 'etc',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everybody',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'everywhere',\n",
       " 'ex',\n",
       " 'exactly',\n",
       " 'example',\n",
       " 'except',\n",
       " 'far',\n",
       " 'few',\n",
       " 'fifth',\n",
       " 'first',\n",
       " 'five',\n",
       " 'followed',\n",
       " 'following',\n",
       " 'follows',\n",
       " 'for',\n",
       " 'former',\n",
       " 'formerly',\n",
       " 'forth',\n",
       " 'four',\n",
       " 'from',\n",
       " 'further',\n",
       " 'furthermore',\n",
       " 'get',\n",
       " 'gets',\n",
       " 'getting',\n",
       " 'given',\n",
       " 'gives',\n",
       " 'go',\n",
       " 'goes',\n",
       " 'going',\n",
       " 'gone',\n",
       " 'got',\n",
       " 'gotten',\n",
       " 'greetings',\n",
       " 'had',\n",
       " \"hadn ' t\",\n",
       " 'happens',\n",
       " 'hardly',\n",
       " 'has',\n",
       " \"hasn ' t\",\n",
       " 'have',\n",
       " \"haven ' t\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'hello',\n",
       " 'help',\n",
       " 'hence',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hereafter',\n",
       " 'hereby',\n",
       " 'herein',\n",
       " \"here ' s\",\n",
       " 'hereupon',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he ' s\",\n",
       " 'hi',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'hither',\n",
       " 'hopefully',\n",
       " 'how',\n",
       " 'howbeit',\n",
       " 'however',\n",
       " \"i ' d\",\n",
       " 'ie',\n",
       " 'if',\n",
       " 'ignored',\n",
       " \"i ' ll\",\n",
       " \"i ' m\",\n",
       " 'immediate',\n",
       " 'in',\n",
       " 'inasmuch',\n",
       " 'inc',\n",
       " 'indeed',\n",
       " 'indicate',\n",
       " 'indicated',\n",
       " 'indicates',\n",
       " 'inner',\n",
       " 'insofar',\n",
       " 'instead',\n",
       " 'into',\n",
       " 'inward',\n",
       " 'is',\n",
       " \"isn ' t\",\n",
       " 'it',\n",
       " \"it ' d\",\n",
       " \"it ' ll\",\n",
       " 'its',\n",
       " \"it ' s\",\n",
       " 'itself',\n",
       " \"i ' ve\",\n",
       " 'just',\n",
       " 'keep',\n",
       " 'keeps',\n",
       " 'kept',\n",
       " 'know',\n",
       " 'known',\n",
       " 'knows',\n",
       " 'last',\n",
       " 'lately',\n",
       " 'later',\n",
       " 'latter',\n",
       " 'latterly',\n",
       " 'least',\n",
       " 'less',\n",
       " 'lest',\n",
       " 'let',\n",
       " \"let ' s\",\n",
       " 'like',\n",
       " 'liked',\n",
       " 'likely',\n",
       " 'little',\n",
       " 'look',\n",
       " 'looking',\n",
       " 'looks',\n",
       " 'ltd',\n",
       " 'mainly',\n",
       " 'many',\n",
       " 'may',\n",
       " 'maybe',\n",
       " 'me',\n",
       " 'mean',\n",
       " 'meanwhile',\n",
       " 'merely',\n",
       " 'might',\n",
       " 'more',\n",
       " 'moreover',\n",
       " 'most',\n",
       " 'mostly',\n",
       " 'much',\n",
       " 'must',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'name',\n",
       " 'namely',\n",
       " 'nd',\n",
       " 'near',\n",
       " 'nearly',\n",
       " 'necessary',\n",
       " 'need',\n",
       " 'needs',\n",
       " 'neither',\n",
       " 'never',\n",
       " 'nevertheless',\n",
       " 'new',\n",
       " 'next',\n",
       " 'nine',\n",
       " 'no',\n",
       " 'nobody',\n",
       " 'non',\n",
       " 'none',\n",
       " 'noone',\n",
       " 'nor',\n",
       " 'normally',\n",
       " 'not',\n",
       " 'nothing',\n",
       " 'novel',\n",
       " 'now',\n",
       " 'nowhere',\n",
       " 'obviously',\n",
       " 'of',\n",
       " 'off',\n",
       " 'often',\n",
       " 'oh',\n",
       " 'ok',\n",
       " 'okay',\n",
       " 'old',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'ones',\n",
       " 'only',\n",
       " 'onto',\n",
       " 'or',\n",
       " 'other',\n",
       " 'others',\n",
       " 'otherwise',\n",
       " 'ought',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'outside',\n",
       " 'over',\n",
       " 'overall',\n",
       " 'own',\n",
       " 'particular',\n",
       " 'particularly',\n",
       " 'per',\n",
       " 'perhaps',\n",
       " 'placed',\n",
       " 'please',\n",
       " 'plus',\n",
       " 'possible',\n",
       " 'presumably',\n",
       " 'probably',\n",
       " 'provides',\n",
       " 'que',\n",
       " 'quite',\n",
       " 'qv',\n",
       " 'rather',\n",
       " 'rd',\n",
       " 're',\n",
       " 'really',\n",
       " 'reasonably',\n",
       " 'regarding',\n",
       " 'regardless',\n",
       " 'regards',\n",
       " 'relatively',\n",
       " 'respectively',\n",
       " 'right',\n",
       " 'said',\n",
       " 'same',\n",
       " 'saw',\n",
       " 'say',\n",
       " 'saying',\n",
       " 'says',\n",
       " 'second',\n",
       " 'secondly',\n",
       " 'see',\n",
       " 'seeing',\n",
       " 'seem',\n",
       " 'seemed',\n",
       " 'seeming',\n",
       " 'seems',\n",
       " 'seen',\n",
       " 'self',\n",
       " 'selves',\n",
       " 'sensible',\n",
       " 'sent',\n",
       " 'serious',\n",
       " 'seriously',\n",
       " 'seven',\n",
       " 'several',\n",
       " 'shall',\n",
       " 'she',\n",
       " 'should',\n",
       " \"shouldn ' t\",\n",
       " 'since',\n",
       " 'six',\n",
       " 'so',\n",
       " 'some',\n",
       " 'somebody',\n",
       " 'somehow',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'sometime',\n",
       " 'sometimes',\n",
       " 'somewhat',\n",
       " 'somewhere',\n",
       " 'soon',\n",
       " 'sorry',\n",
       " 'specified',\n",
       " 'specify',\n",
       " 'specifying',\n",
       " 'still',\n",
       " 'sub',\n",
       " 'such',\n",
       " 'sup',\n",
       " 'sure',\n",
       " 'take',\n",
       " 'taken',\n",
       " 'tell',\n",
       " 'tends',\n",
       " 'th',\n",
       " 'than',\n",
       " 'thank',\n",
       " 'thanks',\n",
       " 'thanx',\n",
       " 'that',\n",
       " 'thats',\n",
       " \"that ' s\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'thence',\n",
       " 'there',\n",
       " 'thereafter',\n",
       " 'thereby',\n",
       " 'therefore',\n",
       " 'therein',\n",
       " 'theres',\n",
       " \"there ' s\",\n",
       " 'thereupon',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they ' d\",\n",
       " \"they ' ll\",\n",
       " \"they ' re\",\n",
       " \"they ' ve\",\n",
       " 'think',\n",
       " 'third',\n",
       " 'this',\n",
       " 'thorough',\n",
       " 'thoroughly',\n",
       " 'those',\n",
       " 'though',\n",
       " 'three',\n",
       " 'through',\n",
       " 'throughout',\n",
       " 'thru',\n",
       " 'thus',\n",
       " 'to',\n",
       " 'together',\n",
       " 'too',\n",
       " 'took',\n",
       " 'toward',\n",
       " 'towards',\n",
       " 'tried',\n",
       " 'tries',\n",
       " 'truly',\n",
       " 'try',\n",
       " 'trying',\n",
       " \"t ' s\",\n",
       " 'twice',\n",
       " 'two',\n",
       " 'un',\n",
       " 'under',\n",
       " 'unfortunately',\n",
       " 'unless',\n",
       " 'unlikely',\n",
       " 'until',\n",
       " 'unto',\n",
       " 'up',\n",
       " 'upon',\n",
       " 'us',\n",
       " 'use',\n",
       " 'used',\n",
       " 'useful',\n",
       " 'uses',\n",
       " 'using',\n",
       " 'usually',\n",
       " 'value',\n",
       " 'various',\n",
       " 'very',\n",
       " 'via',\n",
       " 'viz',\n",
       " 'vs',\n",
       " 'want',\n",
       " 'wants',\n",
       " 'was',\n",
       " \"wasn ' t\",\n",
       " 'way',\n",
       " 'we',\n",
       " \"we ' d\",\n",
       " 'welcome',\n",
       " 'well',\n",
       " \"we ' ll\",\n",
       " 'went',\n",
       " 'were',\n",
       " \"we ' re\",\n",
       " \"weren ' t\",\n",
       " \"we ' ve\",\n",
       " 'what',\n",
       " 'whatever',\n",
       " \"what ' s\",\n",
       " 'when',\n",
       " 'whence',\n",
       " 'whenever',\n",
       " 'where',\n",
       " 'whereafter',\n",
       " 'whereas',\n",
       " 'whereby',\n",
       " 'wherein',\n",
       " \"where ' s\",\n",
       " 'whereupon',\n",
       " 'wherever',\n",
       " 'whether',\n",
       " 'which',\n",
       " 'while',\n",
       " 'whither',\n",
       " 'who',\n",
       " 'whoever',\n",
       " 'whole',\n",
       " 'whom',\n",
       " \"who ' s\",\n",
       " 'whose',\n",
       " 'why',\n",
       " 'will',\n",
       " 'willing',\n",
       " 'wish',\n",
       " 'with',\n",
       " 'within',\n",
       " 'without',\n",
       " 'wonder',\n",
       " \"won ' t\",\n",
       " 'would',\n",
       " \"wouldn ' t\",\n",
       " 'yes',\n",
       " 'yet',\n",
       " 'you',\n",
       " \"you ' d\",\n",
       " \"you ' ll\",\n",
       " 'your',\n",
       " \"you ' re\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you ' ve\",\n",
       " 'zero',\n",
       " 'zt',\n",
       " 'ZT',\n",
       " 'zz',\n",
       " 'ZZ',\n",
       " '一',\n",
       " '一下',\n",
       " '一些',\n",
       " '一切',\n",
       " '一则',\n",
       " '一天',\n",
       " '一定',\n",
       " '一方面',\n",
       " '一旦',\n",
       " '一时',\n",
       " '一来',\n",
       " '一样',\n",
       " '一次',\n",
       " '一片',\n",
       " '一直',\n",
       " '一致',\n",
       " '一般',\n",
       " '一起',\n",
       " '一边',\n",
       " '一面',\n",
       " '万一',\n",
       " '上下',\n",
       " '上升',\n",
       " '上去',\n",
       " '上来',\n",
       " '上述',\n",
       " '上面',\n",
       " '下列',\n",
       " '下去',\n",
       " '下来',\n",
       " '下面',\n",
       " '不 一',\n",
       " '不久',\n",
       " '不仅',\n",
       " '不会',\n",
       " '不但',\n",
       " '不光',\n",
       " '不单',\n",
       " '不变',\n",
       " '不 只',\n",
       " '不可',\n",
       " '不同',\n",
       " '不够',\n",
       " '不如',\n",
       " '不得',\n",
       " '不怕',\n",
       " '不惟',\n",
       " '不成',\n",
       " '不拘',\n",
       " '不敢',\n",
       " '不断',\n",
       " '不是',\n",
       " '不比',\n",
       " '不然',\n",
       " '不 特',\n",
       " '不独',\n",
       " '不管',\n",
       " '不能',\n",
       " '不要',\n",
       " '不论',\n",
       " '不足',\n",
       " '不过',\n",
       " '不问',\n",
       " '与',\n",
       " '与其',\n",
       " '与否',\n",
       " '与此同时',\n",
       " '专门',\n",
       " '且',\n",
       " '两者',\n",
       " '严格',\n",
       " '严重',\n",
       " '个',\n",
       " '个人',\n",
       " '个别',\n",
       " '中小',\n",
       " '中间',\n",
       " '丰富',\n",
       " '临',\n",
       " '为',\n",
       " '为主',\n",
       " '为了',\n",
       " '为什么',\n",
       " '为什 麽',\n",
       " '为何',\n",
       " '为 着',\n",
       " '主张',\n",
       " '主要',\n",
       " '举行',\n",
       " '乃',\n",
       " '乃至',\n",
       " '么',\n",
       " '之',\n",
       " '之一',\n",
       " '之前',\n",
       " '之后',\n",
       " '之 後',\n",
       " '之所以',\n",
       " '之类',\n",
       " '乌乎',\n",
       " '乎',\n",
       " '乘',\n",
       " '也',\n",
       " '也好',\n",
       " '也 是',\n",
       " '也罢',\n",
       " '了',\n",
       " '了解',\n",
       " '争取',\n",
       " '于',\n",
       " '于是',\n",
       " '于是乎',\n",
       " '云云',\n",
       " '互相',\n",
       " '产生',\n",
       " '人们',\n",
       " '人家',\n",
       " '什么',\n",
       " '什么样',\n",
       " '什 麽',\n",
       " '今后',\n",
       " '今天',\n",
       " '今年',\n",
       " '今 後',\n",
       " '仍然',\n",
       " '从',\n",
       " '从事',\n",
       " '从而',\n",
       " '他',\n",
       " '他人',\n",
       " '他们',\n",
       " '他 的',\n",
       " '代替',\n",
       " '以',\n",
       " '以上',\n",
       " '以下',\n",
       " '以为',\n",
       " '以便',\n",
       " '以免',\n",
       " '以前',\n",
       " '以及',\n",
       " '以后',\n",
       " '以外',\n",
       " '以 後',\n",
       " '以来',\n",
       " '以至',\n",
       " '以至于',\n",
       " '以致',\n",
       " '们',\n",
       " '任',\n",
       " '任何',\n",
       " '任凭',\n",
       " '任务',\n",
       " '企图',\n",
       " '伟大',\n",
       " '似乎',\n",
       " '似的',\n",
       " '但',\n",
       " '但是',\n",
       " '何',\n",
       " '何况',\n",
       " '何处',\n",
       " '何时',\n",
       " '作为',\n",
       " '你',\n",
       " '你们',\n",
       " '你 的',\n",
       " '使得',\n",
       " '使用',\n",
       " '例如',\n",
       " '依',\n",
       " '依照',\n",
       " '依靠',\n",
       " '促进',\n",
       " '保持',\n",
       " '俺',\n",
       " '俺们',\n",
       " '倘',\n",
       " '倘使',\n",
       " '倘或',\n",
       " '倘然',\n",
       " '倘若',\n",
       " '假使',\n",
       " '假如',\n",
       " '假若',\n",
       " '做到',\n",
       " '像',\n",
       " '允许',\n",
       " '充分',\n",
       " '先后',\n",
       " '先 後',\n",
       " '先生',\n",
       " '全部',\n",
       " '全面',\n",
       " '兮',\n",
       " '共同',\n",
       " '关于',\n",
       " '其',\n",
       " '其一',\n",
       " '其中',\n",
       " '其二',\n",
       " '其他',\n",
       " '其余',\n",
       " '其它',\n",
       " '其实',\n",
       " '其次',\n",
       " '具体',\n",
       " '具体地说',\n",
       " '具体说来',\n",
       " '具有',\n",
       " '再者',\n",
       " '再说',\n",
       " '冒',\n",
       " '冲',\n",
       " '决定',\n",
       " '况且',\n",
       " '准备',\n",
       " '几',\n",
       " '几乎',\n",
       " '几时',\n",
       " '凭',\n",
       " '凭借',\n",
       " '出去',\n",
       " '出来',\n",
       " '出现',\n",
       " '分别',\n",
       " '则',\n",
       " '别',\n",
       " '别的',\n",
       " '别说',\n",
       " '到',\n",
       " '前后',\n",
       " '前者',\n",
       " '前进',\n",
       " '前面',\n",
       " '加之',\n",
       " '加以',\n",
       " '加入',\n",
       " '加强',\n",
       " '十分',\n",
       " '即',\n",
       " '即令',\n",
       " '即使',\n",
       " '即便',\n",
       " '即 或',\n",
       " '即 若',\n",
       " '却 不',\n",
       " '原来',\n",
       " '又',\n",
       " '及',\n",
       " '及其',\n",
       " '及时',\n",
       " '及至',\n",
       " '双方',\n",
       " '反之',\n",
       " '反应',\n",
       " '反映',\n",
       " '反过来',\n",
       " '反过来说',\n",
       " '取得',\n",
       " '受到',\n",
       " '变成',\n",
       " '另',\n",
       " '另一方面',\n",
       " '另外',\n",
       " '只是',\n",
       " '只有',\n",
       " '只要',\n",
       " '只限',\n",
       " '叫',\n",
       " '叫做',\n",
       " '召开',\n",
       " '叮咚',\n",
       " '可',\n",
       " '可以',\n",
       " '可是',\n",
       " '可能',\n",
       " '可见',\n",
       " '各',\n",
       " '各个',\n",
       " '各人',\n",
       " '各位',\n",
       " '各地',\n",
       " '各种',\n",
       " '各级',\n",
       " '各自',\n",
       " '合理',\n",
       " '同',\n",
       " '同一',\n",
       " '同时',\n",
       " '同样',\n",
       " '后来',\n",
       " '后面',\n",
       " '向',\n",
       " '向着',\n",
       " '吓',\n",
       " '吗',\n",
       " '否则',\n",
       " '吧',\n",
       " '吧哒',\n",
       " '吱',\n",
       " '呀',\n",
       " '呃',\n",
       " '呕',\n",
       " '呗',\n",
       " '呜',\n",
       " '呜呼',\n",
       " '呢',\n",
       " '周围',\n",
       " '呵',\n",
       " '呸',\n",
       " '呼哧',\n",
       " '咋',\n",
       " '和',\n",
       " '咚',\n",
       " '咦',\n",
       " '咱',\n",
       " '咱们',\n",
       " '咳',\n",
       " '哇',\n",
       " '哈',\n",
       " '哈哈',\n",
       " '哉',\n",
       " '哎',\n",
       " '哎呀',\n",
       " '哎哟',\n",
       " '哗',\n",
       " '哟',\n",
       " '哦',\n",
       " '哩',\n",
       " '哪',\n",
       " '哪个',\n",
       " '哪些',\n",
       " '哪儿',\n",
       " '哪 天',\n",
       " '哪年',\n",
       " '哪怕',\n",
       " '哪样',\n",
       " '哪边',\n",
       " '哪里',\n",
       " '哼',\n",
       " '哼唷',\n",
       " '唉',\n",
       " '啊',\n",
       " '啐',\n",
       " '啥',\n",
       " '啦',\n",
       " '啪 达',\n",
       " '喂',\n",
       " '喏',\n",
       " '喔 唷',\n",
       " '嗡嗡',\n",
       " '嗬',\n",
       " '嗯',\n",
       " '嗳',\n",
       " '嘎',\n",
       " '嘎登',\n",
       " '嘘',\n",
       " '嘛',\n",
       " '嘻',\n",
       " '嘿',\n",
       " '因',\n",
       " '因为',\n",
       " '因此',\n",
       " '因而',\n",
       " '固然',\n",
       " '在',\n",
       " '在 下',\n",
       " '地',\n",
       " '坚决',\n",
       " '坚持',\n",
       " '基本',\n",
       " '处理',\n",
       " '复杂',\n",
       " '多',\n",
       " '多少',\n",
       " '多数',\n",
       " '多次',\n",
       " '大力',\n",
       " '大多数',\n",
       " '大大',\n",
       " '大家',\n",
       " '大批',\n",
       " '大约',\n",
       " '大量',\n",
       " '失去',\n",
       " '她',\n",
       " '她们',\n",
       " '她 的',\n",
       " '好 的',\n",
       " '好象',\n",
       " '如',\n",
       " '如上所述',\n",
       " '如下',\n",
       " '如何',\n",
       " '如其',\n",
       " '如果',\n",
       " '如此',\n",
       " '如若',\n",
       " '存在',\n",
       " '宁',\n",
       " '宁可',\n",
       " '宁愿',\n",
       " '宁肯',\n",
       " '它',\n",
       " '它们',\n",
       " '它们 的',\n",
       " '它 的',\n",
       " '安全',\n",
       " '完全',\n",
       " '完成',\n",
       " '实现',\n",
       " '实际',\n",
       " '宣布',\n",
       " '容易',\n",
       " '密切',\n",
       " '对',\n",
       " '对于',\n",
       " '对应',\n",
       " '将',\n",
       " '少数',\n",
       " '尔后',\n",
       " '尚且',\n",
       " '尤其',\n",
       " '就',\n",
       " '就是',\n",
       " '就是说',\n",
       " '尽',\n",
       " '尽管',\n",
       " '属于',\n",
       " '岂但',\n",
       " '左右',\n",
       " '巨大',\n",
       " '巩固',\n",
       " '己',\n",
       " '已经',\n",
       " '帮助',\n",
       " '常常',\n",
       " '并',\n",
       " '并 不',\n",
       " '并 不是',\n",
       " '并且',\n",
       " '并 没有',\n",
       " '广大',\n",
       " '广泛',\n",
       " '应当',\n",
       " '应用',\n",
       " ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b21390a-c846-470d-90d6-d33a5718fd70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hongyi/anaconda3/envs/mystream/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "\u001b[32m2024-11-21 09:37:04.164\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstream_topic.utils.dataset\u001b[0m:\u001b[36mfetch_dataset\u001b[0m:\u001b[36m120\u001b[0m - \u001b[1mFetching dataset: BBC_News\u001b[0m\n",
      "\u001b[32m2024-11-21 09:45:01.386\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstream_topic.utils.data_downloader\u001b[0m:\u001b[36mload_custom_dataset_from_url\u001b[0m:\u001b[36m334\u001b[0m - \u001b[1mDownloading dataset from github\u001b[0m\n",
      "\u001b[32m2024-11-21 09:45:04.135\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstream_topic.utils.data_downloader\u001b[0m:\u001b[36mload_custom_dataset_from_url\u001b[0m:\u001b[36m336\u001b[0m - \u001b[1mDataset downloaded successfully at ~/stream_topic_data/\u001b[0m\n",
      "\u001b[32m2024-11-21 09:45:05.013\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstream_topic.utils.data_downloader\u001b[0m:\u001b[36mload_custom_dataset_from_url\u001b[0m:\u001b[36m364\u001b[0m - \u001b[1mDownloading dataset info from github\u001b[0m\n",
      "\u001b[32m2024-11-21 09:45:06.724\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstream_topic.utils.data_downloader\u001b[0m:\u001b[36mload_custom_dataset_from_url\u001b[0m:\u001b[36m366\u001b[0m - \u001b[1mDataset info downloaded successfully at ~/stream_topic_data/\u001b[0m\n",
      "Preprocessing documents: 100%|█████████████████████████████████████████████████████████████████████████| 2225/2225 [00:06<00:00, 356.10it/s]\n",
      "\u001b[32m2024-11-21 09:45:13.060\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstream_topic.models.KmeansTM\u001b[0m:\u001b[36mfit\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1m--- Training KmeansTM topic model ---\u001b[0m\n",
      "\u001b[32m2024-11-21 09:45:14.974\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstream_topic.models.abstract_helper_models.base\u001b[0m:\u001b[36mprepare_embeddings\u001b[0m:\u001b[36m225\u001b[0m - \u001b[1m--- Creating /hongyi/stream/sentence-transformers/paraphrase-MiniLM-L3-v2/ document embeddings ---\u001b[0m\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 2225/2225 [01:11<00:00, 31.00it/s]\n",
      "\u001b[32m2024-11-21 09:46:27.196\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstream_topic.models.abstract_helper_models.base\u001b[0m:\u001b[36mdim_reduction\u001b[0m:\u001b[36m196\u001b[0m - \u001b[1m--- Reducing dimensions ---\u001b[0m\n",
      "\u001b[32m2024-11-21 09:46:42.277\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstream_topic.models.KmeansTM\u001b[0m:\u001b[36m_clustering\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1m--- Creating document cluster ---\u001b[0m\n",
      "\u001b[32m2024-11-21 09:46:43.507\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstream_topic.models.KmeansTM\u001b[0m:\u001b[36mfit\u001b[0m:\u001b[36m281\u001b[0m - \u001b[1m--- Training completed successfully. ---\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['match', 'england', 'cup', 'champion', 'win', 'coach', 'team', 'injury', 'ireland', 'season'], ['labour', 'election', 'party', 'tory', 'blair', 'lord', 'minister', 'brown', 'prime', 'howard'], ['growth', 'bank', 'economy', 'oil', 'price', 'share', 'market', 'economic', 'china', 'rate'], ['award', 'film', 'actor', 'best', 'star', 'band', 'album', 'song', 'oscar', 'actress'], ['technology', 'mobile', 'phone', 'user', 'computer', 'digital', 'software', 'site', 'network', 'net']]\n"
     ]
    }
   ],
   "source": [
    "from stream_topic.models import KmeansTM\n",
    "from stream_topic.utils import TMDataset\n",
    "\n",
    "dataset = TMDataset()\n",
    "dataset.fetch_dataset(\"BBC_News\")\n",
    "dataset.preprocess(model_type=\"KmeansTM\")\n",
    "\n",
    "model = KmeansTM(\"/hongyi/stream/sentence-transformers/paraphrase-MiniLM-L3-v2/\")\n",
    "model.fit(dataset, n_topics=5)\n",
    "\n",
    "topics = model.get_topics()\n",
    "print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e66a5365-9dde-4595-91a3-dc0c32e1d4fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ask jeeves third leading online firm week internet advertising firm revenue fourth quarter exceeding ask jeeves among relatively modest profit quarter announced google earlier week quarter online advertising taken relatively late last year marketing company doubleclick one leading online advertising warned business would sale thursday announced sharp brought increase profit ask jeeves doubleclick profit news however analyst fall quarter google'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.dataframe.text[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a84e28c5-c67e-4256-aa99-e0f57962eeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stream_topic.metrics import ISIM, INT, ISH,Expressivity, NPMI, Embedding_Coherence, Embedding_Topic_Diversity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from stream_topic.metrics.metrics_config import MetricsConfig\n",
    "MetricsConfig.set_PARAPHRASE_embedder(\"/hongyi/stream/sentence-transformers/paraphrase-MiniLM-L3-v2/\")\n",
    "MetricsConfig.set_SENTENCE_embedder(\"/hongyi/stream/sentence-transformers/all-MiniLM-L6-v2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ebf371a-dce8-4a29-8f59-cfa801943efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from local path: /hongyi/stream/sentence-transformers/paraphrase-MiniLM-L3-v2/\n",
      "Loading model from local path: /hongyi/stream/sentence-transformers/paraphrase-MiniLM-L3-v2/\n",
      "ISIM scores: 0.17864638064056637\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "score_list=[]\n",
    "metric = ISIM()\n",
    "for i in range(100):    \n",
    "    scores = metric.score(topics) #值越小越好\n",
    "    score_list.append(scores)\n",
    "print(\"ISIM scores:\", np.mean(score_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36505835-88c1-4a13-a3ae-ae80ca19611f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'INT' object has no attribute 'metric_embedder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmetric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/STREAM/stream_topic/metrics/intruder_metrics.py:341\u001b[0m, in \u001b[0;36mINT.get_info\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_info\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    327\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;124;03m    Get information about the metric.\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;124;03m        metric range and metric description.\u001b[39;00m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    337\u001b[0m     info \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    338\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetric_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIntruder Topic Metric (INT)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    339\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_words\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_words,\n\u001b[1;32m    340\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_intruders\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_intruders,\n\u001b[0;32m--> 341\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding_model_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetric_embedder\u001b[49m,\n\u001b[1;32m    342\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetric_range\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0 to 1, higher is better\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    343\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe accuracy with which the top words of a topic are least similar to intruder words.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    344\u001b[0m     }\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m info\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'INT' object has no attribute 'metric_embedder'"
     ]
    }
   ],
   "source": [
    "metric.get_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ad59385-4869-4739-b37a-c850debc255c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from local path: /hongyi/stream/sentence-transformers/paraphrase-MiniLM-L3-v2/\n",
      "Loading model from local path: /hongyi/stream/sentence-transformers/paraphrase-MiniLM-L3-v2/\n",
      "INT scores: 0.33999999999999997\n"
     ]
    }
   ],
   "source": [
    "score_list=[]\n",
    "metric = INT()\n",
    "for i in range(100):    \n",
    "    scores = metric.score(topics) #值越小越好\n",
    "    score_list.append(scores)\n",
    "print(\"INT scores:\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31bd176c-f364-4a63-a65d-dcb7b17f7483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from local path: /hongyi/stream/sentence-transformers/paraphrase-MiniLM-L3-v2/\n",
      "Loading model from local path: /hongyi/stream/sentence-transformers/paraphrase-MiniLM-L3-v2/\n",
      "ISH scores: 0.2354140043258667\n"
     ]
    }
   ],
   "source": [
    "score_list=[]\n",
    "metric = ISH()\n",
    "for i in range(100):    \n",
    "    scores = metric.score(topics) #值越小越好\n",
    "    score_list.append(scores)\n",
    "print(\"ISH scores:\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee278df2-91e5-4357-8569-a4825d900359",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "GENSIM_STOPWORDS = gensim.parsing.preprocessing.STOPWORDS\n",
    "NLTK_STOPWORDS = stopwords.words(\"english\")\n",
    "STOPWORDS = list(\n",
    "    set(list(NLTK_STOPWORDS) + list(GENSIM_STOPWORDS) + list(ENGLISH_STOP_WORDS))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84b2256f-dced-4d7f-851c-a2eba6c66dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from local path: /hongyi/stream/sentence-transformers/paraphrase-MiniLM-L3-v2/\n",
      "Loading model from local path: /hongyi/stream/sentence-transformers/paraphrase-MiniLM-L3-v2/\n",
      "Loading model from local path: /hongyi/stream/sentence-transformers/all-MiniLM-L6-v2/\n",
      "Metric Info: {'metric_name': 'Expressivity', 'n_words': 5, 'embedding_model_name': SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "), 'metric_range': '0 to 1, smaller is better', 'description': 'The expressivity metric measures the distance between the mean vector of the top words in a topic and the mean vector of the embeddings of the stop words.'}\n",
      "Expressivity scores: 0.196284\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# topics = [[\"apple\", \"banana\", \"cherry\", \"date\", \"fig\",\"apple\", \"banana\", \"cherry\", \"date\", \"fig\",\"apple\", \"banana\", \"cherry\", \"date\", \"fig\"],\n",
    "#           [\"dog\", \"cat\", \"rabbit\", \"hamster\", \"gerbil\",\"dog\", \"cat\", \"rabbit\", \"hamster\", \"gerbil\",\"dog\", \"cat\", \"rabbit\", \"hamster\", \"gerbil\"]]\n",
    "expressivity_metric = Expressivity(\n",
    "n_words=5,\n",
    "custom_stopwords=STOPWORDS\n",
    ")\n",
    "beta = np.random.rand(5, 384)\n",
    "info = expressivity_metric.get_info()\n",
    "print(\"Metric Info:\", info)\n",
    "scores = expressivity_metric.score(topics, beta)  #值越小越好\n",
    "print(\"Expressivity scores:\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16e8056c-0362-4e6e-9858-6e7652d2bc3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from local path: /hongyi/stream/sentence-transformers/paraphrase-MiniLM-L3-v2/\n",
      "Loading model from local path: /hongyi/stream/sentence-transformers/paraphrase-MiniLM-L3-v2/\n",
      "Metric Info: {'metric_name': 'Embedding Topic Diversity', 'n_words': 10, 'embedding_model_name': SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "), 'metric_range': '0 to 1, smaller is better', 'description': 'The diversity metric measures the mean cosine similarity of the mean vectors of the top words of all topics.'}\n",
      "Diversity score: 0.43775419298273144\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "beta = np.random.rand(5, 384)\n",
    "diversity_metric = Embedding_Topic_Diversity()\n",
    "info = diversity_metric.get_info()\n",
    "print(\"Metric Info:\", info)\n",
    "scores = diversity_metric.score(topics, beta)  #值越小越好\n",
    "print(\"Diversity score:\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66c94ce8-ce15-45aa-8955-f403e3872f9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09477"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = NPMI(dataset,language = \"english\") #值越大越好\n",
    "metric.score(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bb8b9940-b021-4544-bee5-347846b76116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'england, rugby, ireland, france, coach': 0.16290000081062317,\n",
       " 'tax, pension, council, brown, tory': 0.4258599877357483,\n",
       " 'mobile, phone, technology, digital, music': 0.12892000377178192,\n",
       " 'airline, country, aid, economic, budget': 0.3128199875354767,\n",
       " 'band, album, song, music, rock': 0.46094000339508057,\n",
       " 'series, celebrity, comedy, audience, viewer': 0.2819400131702423,\n",
       " 'race, olympic, indoor, championship, holmes': 0.17599999904632568,\n",
       " 'search, broadband, blog, google, net': 0.12077999860048294,\n",
       " 'share, profit, shareholder, financial, executive': 0.22439000010490417,\n",
       " 'chelsea, liverpool, club, league, arsenal': 0.12272000312805176,\n",
       " 'yukos, oil, russian, gazprom, russia': 0.07117000222206116,\n",
       " 'roddick, seed, open, match, nadal': 0.24886000156402588,\n",
       " 'lord, police, law, hunting, trial': 0.4142799973487854,\n",
       " 'labour, party, election, blair, prime': 0.1724500060081482,\n",
       " 'rate, growth, economy, economist, dollar': 0.14369000494480133,\n",
       " 'virus, software, program, email, security': 0.2591400146484375,\n",
       " 'kenteris, thanou, iaaf, conte, test': 0.228970006108284,\n",
       " 'gaming, console, game, nintendo, gamers': 0.22840000689029694,\n",
       " 'film, actor, oscar, award, actress': 0.199070006608963,\n",
       " 'asylum, immigration, health, constitution, child': 0.011070000007748604}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.score_per_topic(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d57125c3-b56d-42da-af07-b30caae2824b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity\n",
    "\n",
    "model_output = {\"topics\": model.get_topics(), \"topic-word-matrix\": model.get_beta(), \"topic-document-matrix\": model.get_theta()}\n",
    "\n",
    "metric = TopicDiversity(topk=10) # Initialize metric\n",
    "topic_diversity_score = metric.score(model_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1985efea-7428-4495-bb28-d8ee0b3c692e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 2225/2225 [01:09<00:00, 31.94it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(dataset.texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0d6ef5c-5c38-4870-b81c-fe589b92125d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "label_mapping = {\n",
    "    'business': 3,\n",
    "    'entertainment': 2,\n",
    "    'politics': 4,\n",
    "    'sport': 1,\n",
    "    'tech': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1629de1-5a3f-45ae-ac92-ce49f6e6ba0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, ..., 3, 3, 3], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9d9a989-a8ac-4f96-a1fc-5dabc6ec21ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# 假设 dataset.labels 包含所有样本的真实标签\n",
    "true_labels = dataset.labels\n",
    "\n",
    "# 将汉字类别转换为数值类别\n",
    "true_labels_numeric = [label_mapping[label] for label in true_labels]\n",
    "\n",
    "# 将预测结果和真实标签转换为 DataFrame\n",
    "df_predictions = pd.DataFrame(predictions, columns=['predicted_labels'])\n",
    "df_true_labels = pd.DataFrame(true_labels_numeric, columns=['true_labels'])\n",
    "\n",
    "# 计算混淆矩阵\n",
    "conf_matrix = confusion_matrix(df_true_labels, df_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33449cec-dc1e-4bbf-95ba-b8bc99edba16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 accuracy: 0.00\n",
      "Topic 1 accuracy: 0.99\n",
      "Topic 2 accuracy: 0.01\n",
      "Topic 3 accuracy: 0.03\n",
      "Topic 4 accuracy: 0.92\n"
     ]
    }
   ],
   "source": [
    "# 计算每个类别的准确率\n",
    "accuracies = {}\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    accuracy = conf_matrix[i, i] / conf_matrix.sum(axis=1)[i]\n",
    "    accuracies[f'Topic {i}'] = accuracy\n",
    "\n",
    "# 打印每个类别的准确率\n",
    "for topic, accuracy in accuracies.items():\n",
    "    print(f'{topic} accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0760df4a-9763-4547-9c63-f28e8d71cbbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[  2   1   1   9 388]\n",
      " [  1 507   0   2   1]\n",
      " [ 14   1 348   2  21]\n",
      " [ 15   0   0 480  15]\n",
      " [384   4   1  26   2]]\n"
     ]
    }
   ],
   "source": [
    "# 如果你想要更详细的信息，可以打印整个混淆矩阵\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mystream",
   "language": "python",
   "name": "mystream"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
