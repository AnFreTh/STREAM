---
library_name: setfit
metrics:
- accuracy
pipeline_tag: text-classification
tags:
- setfit
- sentence-transformers
- text-classification
- generated_from_setfit_trainer
widget:
- text: å¼ å­¦å‹ åŠ¨ç”»ç‰‡ ä¸­æ–°ç½‘ å¼ å­¦å‹ æ–°äººç‹ æ–°äººç‹ å¼ å­¦å‹ å¿ä¸ä½
- text: é«˜è·Ÿé‹ æ—±å†°é‹ ä¸­æ–°ç½‘ å…‹æ—ç‰¹ ç‰¹ä¼å¾· å®‰å‰ä¸½å¨œ åŒ–èº«ä¸º é«˜è·Ÿé‹ æ—±å†°é‹ å®‰å‰ä¸½å¨œ é«˜è·Ÿé‹ æ—±å†°é‹ è€ç…§ç‰‡ é«˜è·Ÿé‹ æ—±å†°é‹
- text: ä¼å¾·æ–¯ å¤§æ‚çƒ© ä¼å¾·æ–¯ å¥¥æ–¯å¡ å¤§æ‚çƒ© æ”¹ç¼–è‡ª å¥¥æ–¯å¡ ç¦å…‹æ–¯ é±¼é±¼æ–‡
- text: ä½“è‚²è®¯ å­£åèµ› åŠå†³èµ› æ³¢å£«é¡¿ å‡¯å°”ç‰¹äºº åŠ æ—¶èµ› è¿ˆé˜¿å¯† å¤§æ¯”åˆ† æ³¢å£«é¡¿ å¾·ç»´æ© ä¸åˆç† æ›´è¡£å®¤ å‡¯å°”ç‰¹äºº ç»“æ„æ€§ æˆ˜æ–—åŠ› å‡¯å°”ç‰¹äºº æ²¡æƒ³åˆ° æˆç»©å•
    ç²¾ç¥åŠ›é‡ å‹åˆ¶ä½ ä¸é€Šäº ç¬¬ä¸€èŠ‚ åŠ å†…ç‰¹ ä»¤äººåƒæƒŠ ç¬¬ä¸€èŠ‚ å‹’å¸ƒæœ— è©¹å§†æ–¯ è©¹å§†æ–¯ è©¹å§†æ–¯ æ²¡æƒ³åˆ° è©¹å§†æ–¯ è©¹å§†æ–¯ ä¸ç”¨è¯´ ä¸‹åŠåœº è©¹å§†æ–¯ ç¬¬å››èŠ‚ äº‹å®ä¸Š
    ç¬¬ä¸‰èŠ‚ ç¬¬å››èŠ‚ å³ä¾¿å¦‚æ­¤ å‡¯å°”ç‰¹äºº
- text: ä½“è‚²è®¯ å­£åèµ› é›„é¹¿é˜Ÿ æ€»ç»ç† å“ˆè’™å¾· å“ˆè’™å¾· å¹´è½»äºº æ€»ç»ç† è¾¾é›·å°”è«é›· ç¬¬ä¸€æ¬¡ å¸ƒé²å…‹æ–¯ å¤ªé˜³é˜Ÿ
inference: true
---

# SetFit

This is a [SetFit](https://github.com/huggingface/setfit) model that can be used for Text Classification. A [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) instance is used for classification.

The model has been trained using an efficient few-shot learning technique that involves:

1. Fine-tuning a [Sentence Transformer](https://www.sbert.net) with contrastive learning.
2. Training a classification head with features from the fine-tuned Sentence Transformer.

## Model Details

### Model Description
- **Model Type:** SetFit
<!-- - **Sentence Transformer:** [Unknown](https://huggingface.co/unknown) -->
- **Classification head:** a [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) instance
- **Maximum Sequence Length:** 128 tokens
- **Number of Classes:** 2 classes
<!-- - **Training Dataset:** [Unknown](https://huggingface.co/datasets/unknown) -->
<!-- - **Language:** Unknown -->
<!-- - **License:** Unknown -->

### Model Sources

- **Repository:** [SetFit on GitHub](https://github.com/huggingface/setfit)
- **Paper:** [Efficient Few-Shot Learning Without Prompts](https://arxiv.org/abs/2209.11055)
- **Blogpost:** [SetFit: Efficient Few-Shot Learning Without Prompts](https://huggingface.co/blog/setfit)

### Model Labels
| Label | Examples                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
|:------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| å¨±ä¹    | <ul><li>'æ–¯å›¾å°”ç‰¹ å…‹é‡Œæ–¯ æ–¯å›¾å°”ç‰¹ å®¶é—¨å£ å¤§éº»çƒŸ å¹´è½»ä¸€ä»£ æ–¯å›¾å°”ç‰¹ æ´›æ‰çŸ¶ å®¶é—¨å£ æ–¯å›¾å°”ç‰¹ å®¶é—¨å£ å¤§éº»çƒŸ æ–¯å›¾å°”ç‰¹ æ–¯å›¾å°”ç‰¹ ç¦æ–¯ç‰¹ ä¸¹å°¼æ–¯ å‡¯ç‘Ÿç³ å‡¯ç‘Ÿç³ ç»“æœæ˜¾ç¤º å‡¯ç‘Ÿç³ æ€»æ”¶å…¥ äººæ°‘å¸ äººæ°‘å¸ å¥½è±å éº¦è€ƒåˆ© éº¦è€ƒåˆ© æ¨¡ç‰¹å„¿ å¯å¡å›  å¯å¡å›  é±¼é±¼æ–‡'</li><li>'ä¸ƒé¾™ç  åª’ä½“æŠ¥é“ çœŸäººç‰ˆ ä¸ƒé¾™ç  ä¸‡ç¾å…ƒ æ„å‘³ç€ ä¸ƒé¾™ç  å¯èƒ½æ€§ å­™æ‚Ÿç©º è©¹å§†æ–¯ æ–¯ç‰¹æ–¯ ä¸ƒé¾™ç  çœŸäººç‰ˆ ç¬¬ä¸€æ‰‹ æ–¯ç‰¹æ–¯ ä¸ƒé¾™ç  æ˜¯å› ä¸º ç¬¬äºŒéƒ¨ æœ‰æ„æ€ æœ‰æ„æ€ ç¬¬äºŒéƒ¨ ä¸ƒé¾™ç  ä¸ƒé¾™ç  ç¬¬ä¸€éƒ¨ ä¸ƒé¾™ç  ç¬¬ä¸€éƒ¨ ç¬¬ä¸€éƒ¨ ç¬¬ä¸€éƒ¨ ç¬¬ä¸€éƒ¨ ä¸€éƒ¨åˆ† ç¬¬äºŒéƒ¨ ä¸ƒé¾™ç  äº‹å®ä¸Š ç¬¬äºŒéƒ¨ ä¸ƒé¾™ç  ä¸ƒé¾™ç  å¿ƒä¹‹æ´›æ–‡'</li><li>'å®‰å‰ä¸½å¨œ å¥½è±å å®‰å‰ä¸½å¨œ å¸ƒè±å¾· è¿”è€è¿˜ç«¥ è®°è€…ä¼š é‡‘åƒå¥– å®‰å‰ä¸½å¨œ ç¦å…‹æ–¯ å®‰å‰ä¸½å¨œ å¥½è±å åŸƒå¤«éš† å“ˆé‡‘æ–¯ å˜‰å¹´å è¿åŠ¨å‘˜ ä¿ä¿æ–‡'</li></ul>                                                                                                                                                                                                                                                                                       |
| ä½“è‚²    | <ul><li>'ä½“è‚²è®¯ å¸¸è§„èµ› å¸¸è§„èµ› æ€»å† å†› åœ£å®‰ä¸œå°¼å¥¥ å¸¸è§„èµ› æ€»å† å†› å¥¥æ‹‰æœ±æ—º å¥¥å°¼å°” ç¬¬äºŒä¸ª å¯èƒ½æ€§ å¾®ä¹å…¶å¾® å³ä¾¿å¦‚æ­¤ å­£åèµ› å¾—åˆ†æ‰‹ å¸¸è§„èµ› è©¹å§†æ–¯ åˆ†æ³¢ä»€ å‡¯å°”ç‰¹äºº çš®å°”æ–¯ çº¦ç¿°é€Š å…‹åŠ³ç¦å¾· è¯ºç»´èŒ¨åŸº æœå…°ç‰¹ åˆ†ç°ç†Š å…°å¤šå¤« åŠ ç´¢å°” ä¸‡åŠ«ä¸å¤ ä¸œè¥¿éƒ¨ è‹±é›„ä¸»ä¹‰'</li><li>'é©¬å¸ƒé‡Œ ä½“è‚²è®¯ å¸¸è§„èµ› ç¬¬äºŒåå…­ è¾½å®é˜Ÿ ä¸¤è¿èƒœ è¾½å®é˜Ÿ éº¦å°”æ–¯ é©¬å¸ƒé‡Œ åˆ˜ä¹¦æ¥  è¾½å®é˜Ÿ æœ‰çš„æ”¾çŸ¢ æ¨æ–‡åš è¾½å®é˜Ÿ ææ™“æ—­ é©¬å¸ƒé‡Œ ç´§è·Ÿç€ é©¬å¸ƒé‡Œ è¾½å®é˜Ÿ ä¸ç”˜ç¤ºå¼± ææ™“æ—­ é”¦ä¸Šæ·»èŠ± ä¸¤åˆ†é’Ÿ ä¸‰åˆ†çƒ ç¬¬äºŒèŠ‚ ä¸‰åˆ†é’Ÿ è¾½å®é˜Ÿ æ¨æ–‡åš è¾½å®é˜Ÿ éº¦å°”æ–¯ é©¬å¸ƒé‡Œ ä¸ŠåŠåœº è¾½å®é˜Ÿ ä¸‹åŠåœº è¾½å®é˜Ÿ ææ™“æ—­ å…ˆæ‹”å¤´ç­¹ ç´§è·Ÿç€ éº¦å°”æ–¯ è¿›ä¸€æ­¥ éº¦å°”æ–¯ é©¬å¸ƒé‡Œ è¾½å®é˜Ÿ é©¬å¸ƒé‡Œ ä¸‰åˆ†çƒ è¾½å®é˜Ÿ æ”¶æ•ˆç”šå¾® å…³é”®æ—¶åˆ» ææ™“æ—­ ä¸Šç¯®å¾—åˆ† ç¨³ä½é˜µè„š ä¸¤åˆ†é’Ÿ é©¬å¸ƒé‡Œ ä¸ªä½æ•° è¾½å®é˜Ÿ ç¬¬å››èŠ‚ æ¨æ–‡åš æœ‰çš„æ”¾çŸ¢ è¾½å®é˜Ÿ ç´§è·Ÿç€ é©¬å¸ƒé‡Œ è¾½å®é˜Ÿ ä¸Šç¯®å¾—åˆ† å…³é”®æ—¶åˆ» ææ™“æ—­ è¾½å®é˜Ÿ è¾½å®é˜Ÿ åˆ˜ä¹¦æ¥  ææ™“æ—­ éº¦å°”æ–¯ é©¬å¸ƒé‡Œ æ¨æ–‡åš'</li><li>'ä½“è‚²è®¯ è¿ˆé˜¿å¯† æ€»æ¯”åˆ† ç¬¬äºŒè½® å‡¯å°”ç‰¹äºº ç³»åˆ—èµ› å±•ç°å‡º ç»Ÿæ²»åŠ› ç¬¬ä¸€èŠ‚ ä¸¤ä½æ•° å¦‚æœè¯´ åŸƒé‡Œå…‹ æ–¯ç‰¹æ‹‰ äº‹å®ä¸Š ç¬¬ä¸€èŠ‚ ç±³å…‹æ–¯ ä¸¤ä½æ•° å®‰ä¸œå°¼ é©¬é‡Œå¥¥ å®‰ä¸œå°¼ ä¸é€Šäº ä¸‰åˆ†çƒ å‘½ä¸­ç‡ æ˜¯å› ä¸º å¤§å¤šæ•° å‹’å¸ƒæœ— è©¹å§†æ–¯ å¾·ç»´æ© å®‰ä¸œå°¼ ç¬¬å››èŠ‚ å®‰ä¸œå°¼ å®‰ä¸œå°¼ å¿ƒç†ç´ è´¨ å®‰ä¸œå°¼ ä¹Ÿå°±æ˜¯è¯´ å‡¯å°”ç‰¹äºº å®‰ä¸œå°¼ å‡¯å°”ç‰¹äºº å®‰ä¸œå°¼ å‡¯å°”ç‰¹äºº ç³»åˆ—èµ› å‡¯å°”ç‰¹äºº ç¬¬ä¸€æ¬¡ å¡æ´›æ–¯ é˜¿ç½—çº¦ è©¹å§†æ–¯ å®‰ä¸œå°¼'</li></ul> |

## Uses

### Direct Use for Inference

First install the SetFit library:

```bash
pip install setfit
```

Then you can load this model and run inference.

```python
from setfit import SetFitModel

# Download from the ğŸ¤— Hub
model = SetFitModel.from_pretrained("setfit_model_id")
# Run inference
preds = model("å¼ å­¦å‹ åŠ¨ç”»ç‰‡ ä¸­æ–°ç½‘ å¼ å­¦å‹ æ–°äººç‹ æ–°äººç‹ å¼ å­¦å‹ å¿ä¸ä½")
```

<!--
### Downstream Use

*List how someone could finetune this model on their own dataset.*
-->

<!--
### Out-of-Scope Use

*List how the model may foreseeably be misused and address what users ought not to do with the model.*
-->

<!--
## Bias, Risks and Limitations

*What are the known or foreseeable issues stemming from this model? You could also flag here known failure cases or weaknesses of the model.*
-->

<!--
### Recommendations

*What are recommendations with respect to the foreseeable issues? For example, filtering explicit content.*
-->

## Training Details

### Training Set Metrics
| Training set | Min | Median  | Max |
|:-------------|:----|:--------|:----|
| Word count   | 1   | 32.1475 | 192 |

| Label | Training Sample Count |
|:------|:----------------------|
| ä½“è‚²    | 402                   |
| å¨±ä¹    | 398                   |

### Training Hyperparameters
- batch_size: (6, 6)
- num_epochs: (10, 10)
- max_steps: -1
- sampling_strategy: oversampling
- num_iterations: 10
- body_learning_rate: (2e-05, 1e-05)
- head_learning_rate: 0.01
- loss: CosineSimilarityLoss
- distance_metric: cosine_distance
- margin: 0.25
- end_to_end: False
- use_amp: False
- warmup_proportion: 0.1
- seed: 42
- eval_max_steps: -1
- load_best_model_at_end: True

### Training Results
| Epoch  | Step | Training Loss | Validation Loss |
|:------:|:----:|:-------------:|:---------------:|
| 0.0004 | 1    | 0.1886        | -               |
| 0.0187 | 50   | 0.2006        | -               |
| 0.0375 | 100  | 0.1118        | -               |
| 0.0562 | 150  | 0.0792        | -               |
| 0.0750 | 200  | 0.0192        | -               |
| 0.0937 | 250  | 0.0042        | -               |
| 0.1125 | 300  | 0.0048        | -               |
| 0.1312 | 350  | 0.0018        | -               |
| 0.1500 | 400  | 0.0009        | -               |
| 0.1687 | 450  | 0.0007        | -               |
| 0.1875 | 500  | 0.0008        | -               |
| 0.2062 | 550  | 0.0003        | -               |
| 0.2250 | 600  | 0.0003        | -               |
| 0.2437 | 650  | 0.0001        | -               |
| 0.2625 | 700  | 0.0001        | -               |
| 0.2812 | 750  | 0.0001        | -               |
| 0.3000 | 800  | 0.0001        | -               |
| 0.3187 | 850  | 0.0001        | -               |
| 0.3375 | 900  | 0.0001        | -               |
| 0.3562 | 950  | 0.0           | -               |
| 0.3750 | 1000 | 0.0001        | -               |
| 0.3937 | 1050 | 0.0001        | -               |
| 0.4124 | 1100 | 0.0001        | -               |
| 0.4312 | 1150 | 0.0           | -               |
| 0.4499 | 1200 | 0.0           | -               |
| 0.4687 | 1250 | 0.0           | -               |
| 0.4874 | 1300 | 0.0           | -               |
| 0.5062 | 1350 | 0.0           | -               |
| 0.5249 | 1400 | 0.0           | -               |
| 0.5437 | 1450 | 0.0           | -               |
| 0.5624 | 1500 | 0.0           | -               |
| 0.5812 | 1550 | 0.0           | -               |
| 0.5999 | 1600 | 0.0           | -               |
| 0.6187 | 1650 | 0.0           | -               |
| 0.6374 | 1700 | 0.0           | -               |
| 0.6562 | 1750 | 0.0           | -               |
| 0.6749 | 1800 | 0.0           | -               |
| 0.6937 | 1850 | 0.0           | -               |
| 0.7124 | 1900 | 0.0           | -               |
| 0.7312 | 1950 | 0.0           | -               |
| 0.7499 | 2000 | 0.0           | -               |
| 0.7687 | 2050 | 0.0           | -               |
| 0.7874 | 2100 | 0.0           | -               |
| 0.8061 | 2150 | 0.0           | -               |
| 0.8249 | 2200 | 0.0           | -               |
| 0.8436 | 2250 | 0.0           | -               |
| 0.8624 | 2300 | 0.0           | -               |
| 0.8811 | 2350 | 0.0           | -               |
| 0.8999 | 2400 | 0.0           | -               |
| 0.9186 | 2450 | 0.0           | -               |
| 0.9374 | 2500 | 0.0           | -               |
| 0.9561 | 2550 | 0.0           | -               |
| 0.9749 | 2600 | 0.0           | -               |
| 0.9936 | 2650 | 0.0           | -               |
| 1.0    | 2667 | -             | 0.0086          |

### Framework Versions
- Python: 3.10.15
- SetFit: 1.0.3
- Sentence Transformers: 3.1.1
- Transformers: 4.40.2
- PyTorch: 2.4.0+cu121
- Datasets: 3.0.2
- Tokenizers: 0.19.1

## Citation

### BibTeX
```bibtex
@article{https://doi.org/10.48550/arxiv.2209.11055,
    doi = {10.48550/ARXIV.2209.11055},
    url = {https://arxiv.org/abs/2209.11055},
    author = {Tunstall, Lewis and Reimers, Nils and Jo, Unso Eun Seo and Bates, Luke and Korat, Daniel and Wasserblat, Moshe and Pereg, Oren},
    keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Efficient Few-Shot Learning Without Prompts},
    publisher = {arXiv},
    year = {2022},
    copyright = {Creative Commons Attribution 4.0 International}
}
```

<!--
## Glossary

*Clearly define terms in order to be accessible across audiences.*
-->

<!--
## Model Card Authors

*Lists the people who create the model card, providing recognition and accountability for the detailed work that goes into its construction.*
-->

<!--
## Model Card Contact

*Provides a way for people who have updates to the Model Card, suggestions, or questions, to contact the Model Card authors.*
-->